{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Player Sub-Roles: Clustering from Per-90 Z-Scores (Path B)\n",
    "\n",
    "Cluster players into sub-roles using the ~75 per-90 z-score columns.\n",
    "Because the feature space is high-dimensional, we first apply PCA to reduce\n",
    "dimensions (retaining 90% variance), then cluster in PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Dynamic path resolution\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for d in docs.iterdir():\n",
    "    if \"Jorge\" in d.name and \"MacBook\" in d.name and d.is_dir():\n",
    "        BASE = d / \"thesis_data\" / \"raw_data\"\n",
    "        NB_DIR = d / \"thesis_data\" / \"notebooks\" / \"player_subroles\"\n",
    "        break\n",
    "\n",
    "# Load prepared z-score data\n",
    "df = pd.read_parquet(NB_DIR / \"player_data_zscores.parquet\")\n",
    "print(f\"Loaded player_data_zscores: {df.shape}\")\n",
    "\n",
    "# Load feature maps\n",
    "with open(NB_DIR / \"feature_maps.json\", \"r\") as f:\n",
    "    feature_maps = json.load(f)\n",
    "\n",
    "position_zscore_features = feature_maps[\"position_zscore_features\"]\n",
    "positions = list(position_zscore_features.keys())\n",
    "print(f\"Positions: {positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results = {}\n",
    "variance_threshold = 0.90\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "fig.suptitle(\"PCA Cumulative Explained Variance (Per-90 Z-Scores)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for idx, pos in enumerate(positions):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    feat_cols = position_zscore_features[pos]\n",
    "    \n",
    "    # Get position data, drop NaN rows\n",
    "    pos_data = df[df[\"from_position\"] == pos].dropna(subset=feat_cols).copy()\n",
    "    X = pos_data[feat_cols].values\n",
    "    \n",
    "    # Standardize (z-scores are already standardized, but re-scale for consistency)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Full PCA\n",
    "    pca_full = PCA(random_state=42)\n",
    "    pca_full.fit(X_scaled)\n",
    "    cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    \n",
    "    # Find n_components for 90% variance\n",
    "    n_comp_90 = np.argmax(cum_var >= variance_threshold) + 1\n",
    "    \n",
    "    # Fit PCA with selected components\n",
    "    pca = PCA(n_components=n_comp_90, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    pca_results[pos] = {\n",
    "        \"pos_data\": pos_data,\n",
    "        \"X_scaled\": X_scaled,\n",
    "        \"X_pca\": X_pca,\n",
    "        \"pca\": pca,\n",
    "        \"scaler\": scaler,\n",
    "        \"n_components\": n_comp_90,\n",
    "        \"cum_var\": cum_var,\n",
    "        \"feature_cols\": feat_cols,\n",
    "    }\n",
    "    \n",
    "    # Plot\n",
    "    n_show = min(30, len(cum_var))\n",
    "    ax.plot(range(1, n_show + 1), cum_var[:n_show], \"o-\", color=\"#4A6FA5\", linewidth=2, markersize=4)\n",
    "    ax.axhline(y=variance_threshold, color=\"#E8724A\", linestyle=\"--\", alpha=0.7, label=f\"90% threshold\")\n",
    "    ax.axvline(x=n_comp_90, color=\"#6B9F6B\", linestyle=\"--\", alpha=0.7, label=f\"n={n_comp_90}\")\n",
    "    ax.set_title(f\"{pos} (n={len(pos_data):,}, features={len(feat_cols)})\", fontsize=10)\n",
    "    ax.set_xlabel(\"# Components\")\n",
    "    ax.set_ylabel(\"Cumulative Variance\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"{pos}: {len(feat_cols)} features -> {n_comp_90} PCA components (90% variance)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = NB_DIR / \"zscores_pca_variance.png\"\n",
    "plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"\\nSaved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_position_pca(X_pca, pos_data, position, k_range=range(2, 9)):\n",
    "    \"\"\"\n",
    "    Cluster players in PCA-reduced feature space.\n",
    "    X_pca is already scaled/reduced.\n",
    "    \"\"\"\n",
    "    # K-means sweep\n",
    "    sil_scores = {}\n",
    "    inertias = {}\n",
    "    for k in k_range:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        labels = km.fit_predict(X_pca)\n",
    "        sil_scores[k] = silhouette_score(X_pca, labels)\n",
    "        inertias[k] = km.inertia_\n",
    "    \n",
    "    best_k = max(sil_scores, key=sil_scores.get)\n",
    "    \n",
    "    # Fit best models\n",
    "    km_best = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
    "    km_labels = km_best.fit_predict(X_pca)\n",
    "    \n",
    "    hc = AgglomerativeClustering(n_clusters=best_k)\n",
    "    hc_labels = hc.fit_predict(X_pca)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=best_k, random_state=42)\n",
    "    gmm_labels = gmm.fit_predict(X_pca)\n",
    "    \n",
    "    return {\n",
    "        \"position\": position,\n",
    "        \"n_players\": len(pos_data),\n",
    "        \"best_k\": best_k,\n",
    "        \"sil_scores\": sil_scores,\n",
    "        \"inertias\": inertias,\n",
    "        \"km_labels\": km_labels,\n",
    "        \"hc_labels\": hc_labels,\n",
    "        \"gmm_labels\": gmm_labels,\n",
    "        \"km_sil\": silhouette_score(X_pca, km_labels),\n",
    "        \"hc_sil\": silhouette_score(X_pca, hc_labels),\n",
    "        \"gmm_sil\": silhouette_score(X_pca, gmm_labels),\n",
    "        \"centers_pca\": km_best.cluster_centers_,\n",
    "        \"pos_data\": pos_data,\n",
    "        \"X_pca\": X_pca,\n",
    "    }\n",
    "\n",
    "\n",
    "cluster_results = {}\n",
    "\n",
    "for pos in positions:\n",
    "    pr = pca_results[pos]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Position: {pos}  (PCA dims: {pr['n_components']})\")\n",
    "    \n",
    "    res = cluster_position_pca(pr[\"X_pca\"], pr[\"pos_data\"], pos)\n",
    "    cluster_results[pos] = res\n",
    "    \n",
    "    print(f\"N players: {res['n_players']:,}\")\n",
    "    print(f\"Best k: {res['best_k']}\")\n",
    "    print(f\"Silhouette:  KMeans={res['km_sil']:.3f}  HC={res['hc_sil']:.3f}  GMM={res['gmm_sil']:.3f}\")\n",
    "    \n",
    "    for method, labels in [(\"KMeans\", res[\"km_labels\"]), (\"HC\", res[\"hc_labels\"]), (\"GMM\", res[\"gmm_labels\"])]:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        sizes = \", \".join([f\"c{u}={c}\" for u, c in zip(unique, counts)])\n",
    "        print(f\"  {method} cluster sizes: {sizes}\")\n",
    "\n",
    "print(f\"\\nClustering complete for {len(cluster_results)} positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "fig.suptitle(\"K-Means Silhouette Score vs. Number of Clusters (Z-Scores + PCA)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for idx, pos in enumerate(positions):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    res = cluster_results[pos]\n",
    "    ks = sorted(res[\"sil_scores\"].keys())\n",
    "    sils = [res[\"sil_scores\"][k] for k in ks]\n",
    "    \n",
    "    ax.plot(ks, sils, \"o-\", color=\"#4A6FA5\", linewidth=2, markersize=6)\n",
    "    ax.plot(res[\"best_k\"], res[\"sil_scores\"][res[\"best_k\"]], \"*\", color=\"#E8724A\",\n",
    "            markersize=18, zorder=5, label=f\"Best k={res['best_k']}\")\n",
    "    ax.set_title(f\"{pos} (n={res['n_players']:,})\", fontsize=11)\n",
    "    ax.set_xlabel(\"k\")\n",
    "    ax.set_ylabel(\"Silhouette Score\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = NB_DIR / \"zscores_silhouette.png\"\n",
    "plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster Profiles (Radar Charts)\n",
    "\n",
    "Since clustering was done in PCA space, we transform the PCA cluster centers back to the\n",
    "original feature space to create interpretable radar charts. We select the top-10 features\n",
    "by variance across clusters for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_radar_chart_zscore(centers_original, feature_labels, position, n_clusters, save_path, max_features=12):\n",
    "    \"\"\"\n",
    "    Create a radar chart from original-scale centers.\n",
    "    If too many features, select the top ones by cross-cluster variance.\n",
    "    \"\"\"\n",
    "    # Select top features by variance across cluster centers\n",
    "    if len(feature_labels) > max_features:\n",
    "        var_across = np.var(centers_original, axis=0)\n",
    "        top_idx = np.argsort(var_across)[::-1][:max_features]\n",
    "        top_idx = np.sort(top_idx)  # keep original order\n",
    "        centers_original = centers_original[:, top_idx]\n",
    "        feature_labels = [feature_labels[i] for i in top_idx]\n",
    "    \n",
    "    n_features = len(feature_labels)\n",
    "    angles = np.linspace(0, 2 * np.pi, n_features, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    colors = [\"#4A6FA5\", \"#E8724A\", \"#6B9F6B\", \"#9B6FA5\", \"#C4A44A\", \"#5AAFAF\", \"#D46A6A\", \"#8B8B8B\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9, 9), subplot_kw=dict(polar=True))\n",
    "    ax.set_title(f\"{position} — Cluster Profiles (Z-Scores, top-{n_features} features)\",\n",
    "                 fontsize=13, fontweight=\"bold\", pad=20)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        values = centers_original[i].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, \"o-\", linewidth=2, label=f\"Cluster {i}\", color=colors[i % len(colors)])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Clean up label names\n",
    "    short_labels = [lbl.replace(\"from_z_score_\", \"\").replace(\" per 90\", \"/90\") for lbl in feature_labels]\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(short_labels, fontsize=7)\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.35, 1.1), fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "\n",
    "for pos in positions:\n",
    "    res = cluster_results[pos]\n",
    "    pr = pca_results[pos]\n",
    "    \n",
    "    # Transform PCA centers back to original feature space\n",
    "    # centers_pca shape: (k, n_components)\n",
    "    # Inverse PCA: X_original_scaled = centers_pca @ pca.components_ + pca.mean_\n",
    "    centers_scaled = pr[\"pca\"].inverse_transform(res[\"centers_pca\"])\n",
    "    centers_original = pr[\"scaler\"].inverse_transform(centers_scaled)\n",
    "    \n",
    "    feature_labels = pr[\"feature_cols\"]\n",
    "    save_path = NB_DIR / f\"zscores_radar_{pos.replace(' ', '_').lower()}.png\"\n",
    "    make_radar_chart_zscore(centers_original, feature_labels, pos, res[\"best_k\"], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_rows = []\n",
    "for pos in positions:\n",
    "    res = cluster_results[pos]\n",
    "    pr = pca_results[pos]\n",
    "    methods_sil = {\"KMeans\": res[\"km_sil\"], \"Hierarchical\": res[\"hc_sil\"], \"GMM\": res[\"gmm_sil\"]}\n",
    "    best_method = max(methods_sil, key=methods_sil.get)\n",
    "    comparison_rows.append({\n",
    "        \"Position\": pos,\n",
    "        \"PCA Dims\": pr[\"n_components\"],\n",
    "        \"Best k\": res[\"best_k\"],\n",
    "        \"KMeans Sil\": round(res[\"km_sil\"], 4),\n",
    "        \"Hierarchical Sil\": round(res[\"hc_sil\"], 4),\n",
    "        \"GMM Sil\": round(res[\"gmm_sil\"], 4),\n",
    "        \"Best Method\": best_method,\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "print(\"=\" * 95)\n",
    "print(\"Method Comparison (Path B: Z-Scores + PCA)\")\n",
    "print(\"=\" * 95)\n",
    "print(comp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cluster Naming and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_dfs = []\n",
    "\n",
    "for pos in positions:\n",
    "    res = cluster_results[pos]\n",
    "    pr = pca_results[pos]\n",
    "    pos_df = pr[\"pos_data\"].copy()\n",
    "    feature_cols = pr[\"feature_cols\"]\n",
    "    best_k = res[\"best_k\"]\n",
    "    \n",
    "    # Transform centers back to original space for naming\n",
    "    centers_scaled = pr[\"pca\"].inverse_transform(res[\"centers_pca\"])\n",
    "    centers_original = pr[\"scaler\"].inverse_transform(centers_scaled)\n",
    "    \n",
    "    # Pick best method's labels\n",
    "    methods_sil = {\"KMeans\": res[\"km_sil\"], \"Hierarchical\": res[\"hc_sil\"], \"GMM\": res[\"gmm_sil\"]}\n",
    "    best_method = max(methods_sil, key=methods_sil.get)\n",
    "    if best_method == \"KMeans\":\n",
    "        labels = res[\"km_labels\"]\n",
    "    elif best_method == \"Hierarchical\":\n",
    "        labels = res[\"hc_labels\"]\n",
    "    else:\n",
    "        labels = res[\"gmm_labels\"]\n",
    "    \n",
    "    pos_df[\"cluster_id\"] = labels\n",
    "    pos_df[\"cluster_method\"] = best_method\n",
    "    \n",
    "    # Analyze cluster profiles\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{pos} (k={best_k}, method={best_method})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    short_names = [c.replace(\"from_z_score_\", \"\").replace(\" per 90\", \"/90\") for c in feature_cols]\n",
    "    cluster_names = []\n",
    "    \n",
    "    for c_id in range(best_k):\n",
    "        center = centers_original[c_id]\n",
    "        sorted_idx = np.argsort(center)[::-1]\n",
    "        top3 = [(short_names[i], center[i]) for i in sorted_idx[:3]]\n",
    "        \n",
    "        top_metric = short_names[sorted_idx[0]]\n",
    "        # Clean up name for label\n",
    "        clean_top = top_metric.replace(\"/90\", \"\").replace(\" %\", \"\").strip()\n",
    "        clean_top = clean_top[:20]  # truncate if too long\n",
    "        name = f\"{pos.split()[0]}_{clean_top.replace(' ', '_')}_{c_id}\"\n",
    "        cluster_names.append(name)\n",
    "        \n",
    "        n_in_cluster = np.sum(labels == c_id)\n",
    "        print(f\"\\n  Cluster {c_id} (n={n_in_cluster:,}): '{name}'\")\n",
    "        print(f\"    Top 3 defining z-score metrics:\")\n",
    "        for m_name, m_val in top3:\n",
    "            print(f\"      {m_name:45s} = {m_val:.3f}\")\n",
    "    \n",
    "    name_map = {i: cluster_names[i] for i in range(best_k)}\n",
    "    pos_df[\"cluster_name\"] = pos_df[\"cluster_id\"].map(name_map)\n",
    "    \n",
    "    all_labeled_dfs.append(pos_df)\n",
    "\n",
    "# Combine\n",
    "df_labeled = pd.concat(all_labeled_dfs, ignore_index=True)\n",
    "\n",
    "# Select output columns\n",
    "id_cols = [\"wy_player_id\", \"from_position\", \"cluster_id\", \"cluster_name\", \"cluster_method\"]\n",
    "extra_id = [\"from_team_id\", \"from_season\", \"from_Minutes\"]\n",
    "name_col = [c for c in df_labeled.columns if c in [\"from_player_name\", \"from_short_name\", \"player_name\"]]\n",
    "zscore_cols_in_df = [c for c in df_labeled.columns if c.startswith(\"from_z_score_\")]\n",
    "\n",
    "out_cols = id_cols + name_col + extra_id + zscore_cols_in_df\n",
    "seen = set()\n",
    "out_cols_unique = []\n",
    "for c in out_cols:\n",
    "    if c not in seen and c in df_labeled.columns:\n",
    "        seen.add(c)\n",
    "        out_cols_unique.append(c)\n",
    "\n",
    "df_export = df_labeled[out_cols_unique].copy()\n",
    "\n",
    "save_path = NB_DIR / \"player_subroles_zscores.parquet\"\n",
    "df_export.to_parquet(save_path, index=False)\n",
    "print(f\"\\nExported: {save_path}\")\n",
    "print(f\"Shape: {df_export.shape}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(df_export.groupby([\"from_position\", \"cluster_name\"]).size().reset_index(name=\"count\").to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}