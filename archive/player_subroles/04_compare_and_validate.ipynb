{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 â€” Compare Clustering Approaches & Validate Sub-Roles\n",
    "\n",
    "Compare Path A (Twelve Football Qualities) vs Path B (Per-90 Z-Scores + PCA) clustering\n",
    "results. Validate with bootstrap stability, UMAP visualization, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Dynamic path resolution\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for d in docs.iterdir():\n",
    "    if \"Jorge\" in d.name and \"MacBook\" in d.name and d.is_dir():\n",
    "        BASE = d / \"thesis_data\" / \"raw_data\"\n",
    "        NB_DIR = d / \"thesis_data\" / \"notebooks\" / \"player_subroles\"\n",
    "        break\n",
    "\n",
    "# Load both clustering results\n",
    "df_qual = pd.read_parquet(NB_DIR / \"player_subroles_qualities.parquet\")\n",
    "df_zsc = pd.read_parquet(NB_DIR / \"player_subroles_zscores.parquet\")\n",
    "\n",
    "# Load original player data (qualities version for features)\n",
    "df_orig_qual = pd.read_parquet(NB_DIR / \"player_data_qualities.parquet\")\n",
    "df_orig_zsc = pd.read_parquet(NB_DIR / \"player_data_zscores.parquet\")\n",
    "\n",
    "# Load feature maps\n",
    "with open(NB_DIR / \"feature_maps.json\", \"r\") as f:\n",
    "    feature_maps = json.load(f)\n",
    "\n",
    "position_quality_features = feature_maps[\"position_quality_features\"]\n",
    "position_zscore_features = feature_maps[\"position_zscore_features\"]\n",
    "\n",
    "positions = list(position_quality_features.keys())\n",
    "\n",
    "print(f\"Loaded Path A (Qualities): {df_qual.shape}\")\n",
    "print(f\"Loaded Path B (Z-Scores):  {df_zsc.shape}\")\n",
    "print(f\"Positions: {positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quantitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Re-compute silhouette scores for both paths\n",
    "comparison_rows = []\n",
    "\n",
    "for pos in positions:\n",
    "    # --- Path A: Qualities ---\n",
    "    feat_cols_a = position_quality_features[pos]\n",
    "    pos_a = df_qual[df_qual[\"from_position\"] == pos].dropna(subset=feat_cols_a).copy()\n",
    "    X_a = pos_a[feat_cols_a].values\n",
    "    scaler_a = StandardScaler()\n",
    "    X_a_scaled = scaler_a.fit_transform(X_a)\n",
    "    labels_a = pos_a[\"cluster_id\"].values\n",
    "    k_a = len(np.unique(labels_a))\n",
    "    sil_a = silhouette_score(X_a_scaled, labels_a)\n",
    "    method_a = pos_a[\"cluster_method\"].iloc[0] if \"cluster_method\" in pos_a.columns else \"Unknown\"\n",
    "    \n",
    "    # --- Path B: Z-Scores + PCA ---\n",
    "    feat_cols_b = position_zscore_features[pos]\n",
    "    pos_b = df_zsc[df_zsc[\"from_position\"] == pos].dropna(subset=feat_cols_b).copy()\n",
    "    X_b = pos_b[feat_cols_b].values\n",
    "    scaler_b = StandardScaler()\n",
    "    X_b_scaled = scaler_b.fit_transform(X_b)\n",
    "    # Apply PCA (90% variance)\n",
    "    pca_b = PCA(random_state=42)\n",
    "    pca_b.fit(X_b_scaled)\n",
    "    cum_var = np.cumsum(pca_b.explained_variance_ratio_)\n",
    "    n_comp = np.argmax(cum_var >= 0.90) + 1\n",
    "    pca_b = PCA(n_components=n_comp, random_state=42)\n",
    "    X_b_pca = pca_b.fit_transform(X_b_scaled)\n",
    "    labels_b = pos_b[\"cluster_id\"].values\n",
    "    k_b = len(np.unique(labels_b))\n",
    "    sil_b = silhouette_score(X_b_pca, labels_b)\n",
    "    method_b = pos_b[\"cluster_method\"].iloc[0] if \"cluster_method\" in pos_b.columns else \"Unknown\"\n",
    "    \n",
    "    comparison_rows.append({\n",
    "        \"Position\": pos,\n",
    "        \"Path A k\": k_a,\n",
    "        \"Path A Method\": method_a,\n",
    "        \"Path A Sil\": round(sil_a, 4),\n",
    "        \"Path B k\": k_b,\n",
    "        \"Path B Method\": method_b,\n",
    "        \"Path B Sil\": round(sil_b, 4),\n",
    "        \"Better Path\": \"A\" if sil_a > sil_b else \"B\",\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "print(\"=\" * 100)\n",
    "print(\"Path A (Qualities) vs Path B (Z-Scores + PCA)\")\n",
    "print(\"=\" * 100)\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(positions))\n",
    "width = 0.35\n",
    "\n",
    "bars_a = ax.bar(x - width/2, comp_df[\"Path A Sil\"], width, label=\"Path A (Qualities)\",\n",
    "                color=\"#4A6FA5\", edgecolor=\"white\", linewidth=0.5)\n",
    "bars_b = ax.bar(x + width/2, comp_df[\"Path B Sil\"], width, label=\"Path B (Z-Scores + PCA)\",\n",
    "                color=\"#E8724A\", edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Position\", fontsize=12)\n",
    "ax.set_ylabel(\"Silhouette Score\", fontsize=12)\n",
    "ax.set_title(\"Silhouette Score Comparison: Qualities vs Z-Scores\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(positions, rotation=20, ha=\"right\", fontsize=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Value labels\n",
    "for bar in bars_a:\n",
    "    ax.annotate(f\"{bar.get_height():.3f}\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 4), textcoords=\"offset points\", ha=\"center\", fontsize=8)\n",
    "for bar in bars_b:\n",
    "    ax.annotate(f\"{bar.get_height():.3f}\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 4), textcoords=\"offset points\", ha=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = NB_DIR / \"path_comparison_silhouette.png\"\n",
    "plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"\\nSaved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cluster Stability (Bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_stability(X, labels, best_k, method=\"KMeans\", n_bootstrap=20, sample_frac=0.80, random_state=42):\n",
    "    \"\"\"\n",
    "    Bootstrap stability: resample 80% of data, recluster, compute ARI vs full labels.\n",
    "    Returns list of ARI values.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n = len(X)\n",
    "    n_sample = int(n * sample_frac)\n",
    "    ari_values = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        idx = rng.choice(n, size=n_sample, replace=False)\n",
    "        X_boot = X[idx]\n",
    "        labels_orig = labels[idx]\n",
    "        \n",
    "        if method == \"KMeans\":\n",
    "            model = KMeans(n_clusters=best_k, n_init=10, random_state=rng.randint(10000))\n",
    "        elif method == \"Hierarchical\":\n",
    "            model = AgglomerativeClustering(n_clusters=best_k)\n",
    "        else:  # GMM\n",
    "            model = GaussianMixture(n_components=best_k, random_state=rng.randint(10000))\n",
    "        \n",
    "        boot_labels = model.fit_predict(X_boot)\n",
    "        ari = adjusted_rand_score(labels_orig, boot_labels)\n",
    "        ari_values.append(ari)\n",
    "    \n",
    "    return ari_values\n",
    "\n",
    "\n",
    "# Run bootstrap for both paths\n",
    "stability_rows = []\n",
    "\n",
    "for pos in positions:\n",
    "    print(f\"\\nBootstrapping {pos}...\")\n",
    "    \n",
    "    # Path A\n",
    "    feat_cols_a = position_quality_features[pos]\n",
    "    pos_a = df_qual[df_qual[\"from_position\"] == pos].dropna(subset=feat_cols_a).copy()\n",
    "    X_a = StandardScaler().fit_transform(pos_a[feat_cols_a].values)\n",
    "    labels_a = pos_a[\"cluster_id\"].values\n",
    "    k_a = len(np.unique(labels_a))\n",
    "    method_a = pos_a[\"cluster_method\"].iloc[0] if \"cluster_method\" in pos_a.columns else \"KMeans\"\n",
    "    \n",
    "    ari_a = bootstrap_stability(X_a, labels_a, k_a, method=method_a)\n",
    "    \n",
    "    # Path B (in PCA space)\n",
    "    feat_cols_b = position_zscore_features[pos]\n",
    "    pos_b = df_zsc[df_zsc[\"from_position\"] == pos].dropna(subset=feat_cols_b).copy()\n",
    "    X_b_raw = StandardScaler().fit_transform(pos_b[feat_cols_b].values)\n",
    "    pca_tmp = PCA(random_state=42)\n",
    "    pca_tmp.fit(X_b_raw)\n",
    "    cum_var = np.cumsum(pca_tmp.explained_variance_ratio_)\n",
    "    n_comp = np.argmax(cum_var >= 0.90) + 1\n",
    "    X_b = PCA(n_components=n_comp, random_state=42).fit_transform(X_b_raw)\n",
    "    labels_b = pos_b[\"cluster_id\"].values\n",
    "    k_b = len(np.unique(labels_b))\n",
    "    method_b = pos_b[\"cluster_method\"].iloc[0] if \"cluster_method\" in pos_b.columns else \"KMeans\"\n",
    "    \n",
    "    ari_b = bootstrap_stability(X_b, labels_b, k_b, method=method_b)\n",
    "    \n",
    "    stability_rows.append({\n",
    "        \"Position\": pos,\n",
    "        \"Path A ARI (mean)\": round(np.mean(ari_a), 4),\n",
    "        \"Path A ARI (std)\": round(np.std(ari_a), 4),\n",
    "        \"Path B ARI (mean)\": round(np.mean(ari_b), 4),\n",
    "        \"Path B ARI (std)\": round(np.std(ari_b), 4),\n",
    "        \"More Stable\": \"A\" if np.mean(ari_a) > np.mean(ari_b) else \"B\",\n",
    "    })\n",
    "    \n",
    "    print(f\"  Path A: ARI={np.mean(ari_a):.4f} +/- {np.std(ari_a):.4f}\")\n",
    "    print(f\"  Path B: ARI={np.mean(ari_b):.4f} +/- {np.std(ari_b):.4f}\")\n",
    "\n",
    "stab_df = pd.DataFrame(stability_rows)\n",
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"Bootstrap Cluster Stability (20 iterations, 80% sample)\")\n",
    "print(\"=\" * 85)\n",
    "print(stab_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        import umap.umap_ as umap\n",
    "        HAS_UMAP = True\n",
    "    except ImportError:\n",
    "        HAS_UMAP = False\n",
    "        print(\"UMAP not installed. Falling back to t-SNE for visualization.\")\n",
    "\n",
    "if not HAS_UMAP:\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "# Choose the best path for each position based on silhouette comparison\n",
    "best_path = {}\n",
    "for _, row in comp_df.iterrows():\n",
    "    best_path[row[\"Position\"]] = row[\"Better Path\"]\n",
    "\n",
    "colors_map = [\"#4A6FA5\", \"#E8724A\", \"#6B9F6B\", \"#9B6FA5\", \"#C4A44A\", \"#5AAFAF\", \"#D46A6A\", \"#8B8B8B\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "fig.suptitle(\"2D Projection of Player Sub-Roles (Best Path per Position)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for idx, pos in enumerate(positions):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    path = best_path[pos]\n",
    "    \n",
    "    if path == \"A\":\n",
    "        feat_cols = position_quality_features[pos]\n",
    "        pos_df = df_qual[df_qual[\"from_position\"] == pos].dropna(subset=feat_cols).copy()\n",
    "        X = StandardScaler().fit_transform(pos_df[feat_cols].values)\n",
    "        labels = pos_df[\"cluster_id\"].values\n",
    "        path_label = \"Qualities\"\n",
    "    else:\n",
    "        feat_cols = position_zscore_features[pos]\n",
    "        pos_df = df_zsc[df_zsc[\"from_position\"] == pos].dropna(subset=feat_cols).copy()\n",
    "        X_raw = StandardScaler().fit_transform(pos_df[feat_cols].values)\n",
    "        pca_tmp = PCA(random_state=42)\n",
    "        pca_tmp.fit(X_raw)\n",
    "        cum_var = np.cumsum(pca_tmp.explained_variance_ratio_)\n",
    "        n_comp = np.argmax(cum_var >= 0.90) + 1\n",
    "        X = PCA(n_components=n_comp, random_state=42).fit_transform(X_raw)\n",
    "        labels = pos_df[\"cluster_id\"].values\n",
    "        path_label = \"Z-Scores+PCA\"\n",
    "    \n",
    "    # 2D projection\n",
    "    if HAS_UMAP:\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=30, min_dist=0.3)\n",
    "        embedding = reducer.fit_transform(X)\n",
    "        method_name = \"UMAP\"\n",
    "    else:\n",
    "        # Use a subsample for t-SNE if data is large\n",
    "        max_tsne = 10000\n",
    "        if len(X) > max_tsne:\n",
    "            rng = np.random.RandomState(42)\n",
    "            sample_idx = rng.choice(len(X), size=max_tsne, replace=False)\n",
    "            X_sub = X[sample_idx]\n",
    "            labels_sub = labels[sample_idx]\n",
    "        else:\n",
    "            X_sub = X\n",
    "            labels_sub = labels\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        embedding = reducer.fit_transform(X_sub)\n",
    "        labels = labels_sub\n",
    "        method_name = \"t-SNE\"\n",
    "    \n",
    "    # Scatter plot\n",
    "    unique_labels = np.unique(labels)\n",
    "    for i, lab in enumerate(unique_labels):\n",
    "        mask = labels == lab\n",
    "        ax.scatter(embedding[mask, 0], embedding[mask, 1], c=colors_map[i % len(colors_map)],\n",
    "                   label=f\"Cluster {lab}\", alpha=0.5, s=8, edgecolors=\"none\")\n",
    "    \n",
    "    ax.set_title(f\"{pos} (Path {path}: {path_label})\", fontsize=11)\n",
    "    ax.set_xlabel(f\"{method_name} 1\", fontsize=9)\n",
    "    ax.set_ylabel(f\"{method_name} 2\", fontsize=9)\n",
    "    ax.legend(fontsize=8, markerscale=2)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = NB_DIR / \"umap_subroles.png\"\n",
    "plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance (XGBoost / Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try XGBoost, fall back to RandomForest\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    USE_XGB = True\n",
    "    print(\"Using XGBoost for feature importance.\")\n",
    "except ImportError:\n",
    "    USE_XGB = False\n",
    "    print(\"XGBoost not installed. Using RandomForest for feature importance.\")\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Use quality features (Path A) for interpretability\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "fig.suptitle(\"Feature Importance for Cluster Prediction (Quality Features)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "importance_results = {}\n",
    "\n",
    "for idx, pos in enumerate(positions):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    feat_cols = position_quality_features[pos]\n",
    "    pos_df = df_qual[df_qual[\"from_position\"] == pos].dropna(subset=feat_cols).copy()\n",
    "    X = pos_df[feat_cols].values\n",
    "    y = pos_df[\"cluster_id\"].values\n",
    "    \n",
    "    if len(np.unique(y)) < 2:\n",
    "        ax.set_title(f\"{pos} (only 1 cluster, skipped)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        if USE_XGB:\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=100, max_depth=4, random_state=42,\n",
    "                eval_metric=\"mlogloss\", verbosity=0,\n",
    "                use_label_encoder=False\n",
    "            )\n",
    "        else:\n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Sort by importance\n",
    "        sorted_idx = np.argsort(importances)[::-1]\n",
    "        top_n = min(10, len(feat_cols))\n",
    "        top_idx = sorted_idx[:top_n]\n",
    "        \n",
    "        short_names = [c.replace(\"from_\", \"\") for c in feat_cols]\n",
    "        top_names = [short_names[i] for i in top_idx]\n",
    "        top_imps = [importances[i] for i in top_idx]\n",
    "        \n",
    "        importance_results[pos] = list(zip(top_names, top_imps))\n",
    "        \n",
    "        # Horizontal bar chart\n",
    "        y_pos = np.arange(top_n)\n",
    "        ax.barh(y_pos, top_imps[::-1], color=\"#4A6FA5\", edgecolor=\"white\", linewidth=0.5)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(top_names[::-1], fontsize=9)\n",
    "        ax.set_xlabel(\"Importance\", fontsize=9)\n",
    "        ax.set_title(f\"{pos} (k={len(np.unique(y))})\", fontsize=11)\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "        \n",
    "    except Exception as e:\n",
    "        ax.set_title(f\"{pos} (error: {str(e)[:40]})\")\n",
    "        print(f\"Error for {pos}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = NB_DIR / \"feature_importance.png\"\n",
    "plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"FINAL RECOMMENDATION PER POSITION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "final_rows = []\n",
    "\n",
    "for i, pos in enumerate(positions):\n",
    "    row_comp = comp_df.iloc[i]\n",
    "    row_stab = stab_df.iloc[i]\n",
    "    \n",
    "    # Decision logic: prefer higher silhouette; if close, prefer higher stability\n",
    "    sil_a = row_comp[\"Path A Sil\"]\n",
    "    sil_b = row_comp[\"Path B Sil\"]\n",
    "    ari_a = row_stab[\"Path A ARI (mean)\"]\n",
    "    ari_b = row_stab[\"Path B ARI (mean)\"]\n",
    "    \n",
    "    # If silhouette difference is small (<0.02), use stability as tiebreaker\n",
    "    if abs(sil_a - sil_b) < 0.02:\n",
    "        recommended_path = \"A\" if ari_a >= ari_b else \"B\"\n",
    "        reason = \"stability tiebreaker\"\n",
    "    else:\n",
    "        recommended_path = \"A\" if sil_a > sil_b else \"B\"\n",
    "        reason = \"higher silhouette\"\n",
    "    \n",
    "    if recommended_path == \"A\":\n",
    "        rec_k = row_comp[\"Path A k\"]\n",
    "        rec_method = row_comp[\"Path A Method\"]\n",
    "        rec_sil = sil_a\n",
    "        rec_ari = ari_a\n",
    "        src_df = df_qual\n",
    "    else:\n",
    "        rec_k = row_comp[\"Path B k\"]\n",
    "        rec_method = row_comp[\"Path B Method\"]\n",
    "        rec_sil = sil_b\n",
    "        rec_ari = ari_b\n",
    "        src_df = df_zsc\n",
    "    \n",
    "    # Get cluster names and sizes\n",
    "    pos_data = src_df[src_df[\"from_position\"] == pos]\n",
    "    cluster_info = pos_data.groupby([\"cluster_id\", \"cluster_name\"]).size().reset_index(name=\"count\")\n",
    "    \n",
    "    print(f\"\\n--- {pos} ---\")\n",
    "    print(f\"  Recommended: Path {recommended_path} ({reason})\")\n",
    "    print(f\"  Method: {rec_method}, k={rec_k}\")\n",
    "    print(f\"  Silhouette: {rec_sil:.4f}, Bootstrap ARI: {rec_ari:.4f}\")\n",
    "    print(f\"  Sub-roles:\")\n",
    "    for _, cr in cluster_info.iterrows():\n",
    "        print(f\"    [{cr['cluster_id']}] {cr['cluster_name']:40s} n={cr['count']:,}\")\n",
    "    \n",
    "    for _, cr in cluster_info.iterrows():\n",
    "        final_rows.append({\n",
    "            \"position\": pos,\n",
    "            \"recommended_path\": recommended_path,\n",
    "            \"method\": rec_method,\n",
    "            \"k\": rec_k,\n",
    "            \"silhouette\": rec_sil,\n",
    "            \"bootstrap_ari\": rec_ari,\n",
    "            \"cluster_id\": cr[\"cluster_id\"],\n",
    "            \"cluster_name\": cr[\"cluster_name\"],\n",
    "            \"n_players\": cr[\"count\"],\n",
    "        })\n",
    "\n",
    "# Build final combined parquet\n",
    "# For each position, take data from the recommended path\n",
    "final_dfs = []\n",
    "for pos in positions:\n",
    "    recommended_path = best_path[pos]\n",
    "    # Re-check with stability tiebreaker\n",
    "    row_comp_i = comp_df[comp_df[\"Position\"] == pos].iloc[0]\n",
    "    row_stab_i = stab_df[stab_df[\"Position\"] == pos].iloc[0]\n",
    "    sil_a = row_comp_i[\"Path A Sil\"]\n",
    "    sil_b = row_comp_i[\"Path B Sil\"]\n",
    "    ari_a = row_stab_i[\"Path A ARI (mean)\"]\n",
    "    ari_b = row_stab_i[\"Path B ARI (mean)\"]\n",
    "    if abs(sil_a - sil_b) < 0.02:\n",
    "        rec_path = \"A\" if ari_a >= ari_b else \"B\"\n",
    "    else:\n",
    "        rec_path = \"A\" if sil_a > sil_b else \"B\"\n",
    "    \n",
    "    if rec_path == \"A\":\n",
    "        pos_data = df_qual[df_qual[\"from_position\"] == pos].copy()\n",
    "    else:\n",
    "        pos_data = df_zsc[df_zsc[\"from_position\"] == pos].copy()\n",
    "    \n",
    "    pos_data[\"recommended_path\"] = rec_path\n",
    "    final_dfs.append(pos_data)\n",
    "\n",
    "df_final = pd.concat(final_dfs, ignore_index=True)\n",
    "\n",
    "# Keep essential columns\n",
    "essential = [\"wy_player_id\", \"from_position\", \"from_team_id\", \"from_season\", \"from_Minutes\",\n",
    "             \"cluster_id\", \"cluster_name\", \"cluster_method\", \"recommended_path\"]\n",
    "# Add player name if present\n",
    "for nc in [\"from_player_name\", \"from_short_name\", \"player_name\"]:\n",
    "    if nc in df_final.columns:\n",
    "        essential.insert(1, nc)\n",
    "        break\n",
    "\n",
    "available = [c for c in essential if c in df_final.columns]\n",
    "df_final_out = df_final[available].copy()\n",
    "\n",
    "save_path = NB_DIR / \"player_subroles_final.parquet\"\n",
    "df_final_out.to_parquet(save_path, index=False)\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"Final output saved: {save_path}\")\n",
    "print(f\"Shape: {df_final_out.shape}\")\n",
    "print(f\"\\nSummary:\")\n",
    "summary = df_final_out.groupby([\"from_position\", \"cluster_name\"]).size().reset_index(name=\"count\")\n",
    "print(summary.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}