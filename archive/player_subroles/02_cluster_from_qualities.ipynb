{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Player Sub-Roles: Clustering from Qualities (Path A)\n",
    "\n",
    "Cluster players into sub-roles using the 20 pre-computed Twelve Football quality scores.\n",
    "For each position we sweep K-means k=2..8, pick the best k by silhouette, then also fit\n",
    "Hierarchical (Ward) and GMM at that k for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Dynamic path resolution\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for d in docs.iterdir():\n",
    "    if \"Jorge\" in d.name and \"MacBook\" in d.name and d.is_dir():\n",
    "        BASE = d / \"thesis_data\" / \"raw_data\"\n",
    "        NB_DIR = d / \"thesis_data\" / \"notebooks\" / \"player_subroles\"\n",
    "        break\n",
    "\n",
    "# Load prepared quality data\n",
    "df = pd.read_parquet(NB_DIR / \"player_data_qualities.parquet\")\n",
    "print(f\"Loaded player_data_qualities: {df.shape}\")\n",
    "\n",
    "# Load feature maps\n",
    "with open(NB_DIR / \"feature_maps.json\", \"r\") as f:\n",
    "    feature_maps = json.load(f)\n",
    "\n",
    "position_quality_features = feature_maps[\"position_quality_features\"]\n",
    "positions = list(position_quality_features.keys())\n",
    "print(f\"Positions: {positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_position(df, position, feature_cols, k_range=range(2, 9)):\n",
    "    \"\"\"\n",
    "    Cluster players within a position using their quality features.\n",
    "    \n",
    "    Returns a dict with cluster labels, scores, centers, and metadata.\n",
    "    \"\"\"\n",
    "    pos_df = df[df[\"from_position\"] == position].dropna(subset=feature_cols).copy()\n",
    "    X = pos_df[feature_cols].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # K-means sweep\n",
    "    sil_scores = {}\n",
    "    inertias = {}\n",
    "    for k in k_range:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        labels = km.fit_predict(X_scaled)\n",
    "        sil_scores[k] = silhouette_score(X_scaled, labels)\n",
    "        inertias[k] = km.inertia_\n",
    "    \n",
    "    best_k = max(sil_scores, key=sil_scores.get)\n",
    "    \n",
    "    # Fit best models\n",
    "    km_best = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
    "    km_labels = km_best.fit_predict(X_scaled)\n",
    "    \n",
    "    hc = AgglomerativeClustering(n_clusters=best_k)\n",
    "    hc_labels = hc.fit_predict(X_scaled)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=best_k, random_state=42)\n",
    "    gmm_labels = gmm.fit_predict(X_scaled)\n",
    "    \n",
    "    return {\n",
    "        \"position\": position,\n",
    "        \"n_players\": len(pos_df),\n",
    "        \"best_k\": best_k,\n",
    "        \"sil_scores\": sil_scores,\n",
    "        \"inertias\": inertias,\n",
    "        \"km_labels\": km_labels,\n",
    "        \"hc_labels\": hc_labels,\n",
    "        \"gmm_labels\": gmm_labels,\n",
    "        \"km_sil\": silhouette_score(X_scaled, km_labels),\n",
    "        \"hc_sil\": silhouette_score(X_scaled, hc_labels),\n",
    "        \"gmm_sil\": silhouette_score(X_scaled, gmm_labels),\n",
    "        \"centers\": km_best.cluster_centers_,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"scaler\": scaler,\n",
    "        \"pos_df\": pos_df,\n",
    "        \"X_scaled\": X_scaled,\n",
    "    }\n",
    "\n",
    "print(\"Clustering pipeline defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Clustering for Each Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for pos in positions:\n",
    "    feat_cols = position_quality_features[pos]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Position: {pos}\")\n",
    "    print(f\"Features ({len(feat_cols)}): {[c.replace('from_', '') for c in feat_cols]}\")\n",
    "    \n",
    "    res = cluster_position(df, pos, feat_cols)\n",
    "    results[pos] = res\n",
    "    \n",
    "    print(f\"N players (after dropping NaN): {res['n_players']:,}\")\n",
    "    print(f\"Best k: {res['best_k']}\")\n",
    "    print(f\"Silhouette scores:  KMeans={res['km_sil']:.3f}  HC={res['hc_sil']:.3f}  GMM={res['gmm_sil']:.3f}\")\n",
    "    \n",
    "    # Cluster sizes\n",
    "    for method, labels in [(\"KMeans\", res[\"km_labels\"]), (\"HC\", res[\"hc_labels\"]), (\"GMM\", res[\"gmm_labels\"])]:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        sizes = \", \".join([f\"c{u}={c}\" for u, c in zip(unique, counts)])\n",
    "        print(f\"  {method} cluster sizes: {sizes}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Clustering complete for {len(results)} positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "fig.suptitle(\"K-Means Silhouette Score vs. Number of Clusters (Qualities)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for idx, pos in enumerate(positions):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    res = results[pos]\n",
    "    ks = sorted(res[\"sil_scores\"].keys())\n",
    "    sils = [res[\"sil_scores\"][k] for k in ks]\n",
    "    \n",
    "    ax.plot(ks, sils, \"o-\", color=\"#4A6FA5\", linewidth=2, markersize=6)\n",
    "    ax.plot(res[\"best_k\"], res[\"sil_scores\"][res[\"best_k\"]], \"*\", color=\"#E8724A\",\n",
    "            markersize=18, zorder=5, label=f\"Best k={res['best_k']}\")\n",
    "    ax.set_title(f\"{pos} (n={res['n_players']:,})\", fontsize=11)\n",
    "    ax.set_xlabel(\"k\")\n",
    "    ax.set_ylabel(\"Silhouette Score\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = NB_DIR / \"qualities_silhouette.png\"\n",
    "plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster Profiles (Radar Charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_radar_chart(centers_original, feature_labels, position, n_clusters, save_path):\n",
    "    \"\"\"\n",
    "    Create a radar chart for cluster centers (in original scale).\n",
    "    \"\"\"\n",
    "    n_features = len(feature_labels)\n",
    "    angles = np.linspace(0, 2 * np.pi, n_features, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # close the polygon\n",
    "    \n",
    "    # Color palette (blue-gray tones)\n",
    "    colors = [\"#4A6FA5\", \"#E8724A\", \"#6B9F6B\", \"#9B6FA5\", \"#C4A44A\", \"#5AAFAF\", \"#D46A6A\", \"#8B8B8B\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    ax.set_title(f\"{position} — Cluster Profiles (Qualities)\", fontsize=13, fontweight=\"bold\", pad=20)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        values = centers_original[i].tolist()\n",
    "        values += values[:1]  # close\n",
    "        ax.plot(angles, values, \"o-\", linewidth=2, label=f\"Cluster {i}\", color=colors[i % len(colors)])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Labels\n",
    "    short_labels = [lbl.replace(\"from_\", \"\") for lbl in feature_labels]\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(short_labels, fontsize=8)\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "\n",
    "for pos in positions:\n",
    "    res = results[pos]\n",
    "    # Transform centers back to original scale\n",
    "    centers_original = res[\"scaler\"].inverse_transform(res[\"centers\"])\n",
    "    feature_labels = res[\"feature_cols\"]\n",
    "    \n",
    "    save_path = NB_DIR / f\"qualities_radar_{pos.replace(' ', '_').lower()}.png\"\n",
    "    make_radar_chart(centers_original, feature_labels, pos, res[\"best_k\"], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_rows = []\n",
    "for pos in positions:\n",
    "    res = results[pos]\n",
    "    methods_sil = {\"KMeans\": res[\"km_sil\"], \"Hierarchical\": res[\"hc_sil\"], \"GMM\": res[\"gmm_sil\"]}\n",
    "    best_method = max(methods_sil, key=methods_sil.get)\n",
    "    comparison_rows.append({\n",
    "        \"Position\": pos,\n",
    "        \"Best k\": res[\"best_k\"],\n",
    "        \"KMeans Sil\": round(res[\"km_sil\"], 4),\n",
    "        \"Hierarchical Sil\": round(res[\"hc_sil\"], 4),\n",
    "        \"GMM Sil\": round(res[\"gmm_sil\"], 4),\n",
    "        \"Best Method\": best_method,\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "print(\"=\" * 85)\n",
    "print(\"Method Comparison (Path A: Qualities)\")\n",
    "print(\"=\" * 85)\n",
    "print(comp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cluster Naming and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each position, analyze cluster centers and suggest names\n",
    "all_labeled_dfs = []\n",
    "\n",
    "for pos in positions:\n",
    "    res = results[pos]\n",
    "    pos_df = res[\"pos_df\"].copy()\n",
    "    feature_cols = res[\"feature_cols\"]\n",
    "    centers_original = res[\"scaler\"].inverse_transform(res[\"centers\"])\n",
    "    best_k = res[\"best_k\"]\n",
    "    \n",
    "    # Pick the best method's labels\n",
    "    methods_sil = {\"KMeans\": res[\"km_sil\"], \"Hierarchical\": res[\"hc_sil\"], \"GMM\": res[\"gmm_sil\"]}\n",
    "    best_method = max(methods_sil, key=methods_sil.get)\n",
    "    if best_method == \"KMeans\":\n",
    "        labels = res[\"km_labels\"]\n",
    "    elif best_method == \"Hierarchical\":\n",
    "        labels = res[\"hc_labels\"]\n",
    "    else:\n",
    "        labels = res[\"gmm_labels\"]\n",
    "    \n",
    "    pos_df[\"cluster_id\"] = labels\n",
    "    pos_df[\"cluster_method\"] = best_method\n",
    "    \n",
    "    # Analyze cluster profiles: top 3 qualities per cluster (by center value)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{pos} (k={best_k}, method={best_method})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    short_names = [c.replace(\"from_\", \"\") for c in feature_cols]\n",
    "    cluster_names = []\n",
    "    \n",
    "    for c_id in range(best_k):\n",
    "        center = centers_original[c_id]\n",
    "        # Rank features by value (higher = more defining)\n",
    "        sorted_idx = np.argsort(center)[::-1]\n",
    "        top3 = [(short_names[i], center[i]) for i in sorted_idx[:3]]\n",
    "        \n",
    "        # Generate a descriptive name from top qualities\n",
    "        top_quality = short_names[sorted_idx[0]]\n",
    "        second_quality = short_names[sorted_idx[1]]\n",
    "        \n",
    "        # Heuristic naming based on dominant quality combinations\n",
    "        name = f\"{pos.split()[0]}_{top_quality.replace(' ', '_')}_{c_id}\"\n",
    "        cluster_names.append(name)\n",
    "        \n",
    "        n_in_cluster = np.sum(labels == c_id)\n",
    "        print(f\"\\n  Cluster {c_id} (n={n_in_cluster:,}): '{name}'\")\n",
    "        print(f\"    Top 3 defining qualities:\")\n",
    "        for q_name, q_val in top3:\n",
    "            print(f\"      {q_name:30s} = {q_val:.2f}\")\n",
    "    \n",
    "    # Map cluster names\n",
    "    name_map = {i: cluster_names[i] for i in range(best_k)}\n",
    "    pos_df[\"cluster_name\"] = pos_df[\"cluster_id\"].map(name_map)\n",
    "    \n",
    "    all_labeled_dfs.append(pos_df)\n",
    "\n",
    "# Combine all positions\n",
    "df_labeled = pd.concat(all_labeled_dfs, ignore_index=True)\n",
    "\n",
    "# Select output columns\n",
    "id_cols = [\"wy_player_id\", \"from_position\", \"cluster_id\", \"cluster_name\", \"cluster_method\"]\n",
    "# Add all quality columns\n",
    "all_qual_cols = [c for c in df_labeled.columns if c.startswith(\"from_\") and c != \"from_position\" and c != \"from_Minutes\"]\n",
    "# Also keep from_team_id, from_season, from_Minutes\n",
    "extra_id = [\"from_team_id\", \"from_season\", \"from_Minutes\"]\n",
    "# Check for player name\n",
    "name_col = [c for c in df_labeled.columns if c in [\"from_player_name\", \"from_short_name\", \"player_name\"]]\n",
    "\n",
    "out_cols = id_cols + name_col + extra_id + all_qual_cols\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "out_cols_unique = []\n",
    "for c in out_cols:\n",
    "    if c not in seen and c in df_labeled.columns:\n",
    "        seen.add(c)\n",
    "        out_cols_unique.append(c)\n",
    "\n",
    "df_export = df_labeled[out_cols_unique].copy()\n",
    "\n",
    "save_path = NB_DIR / \"player_subroles_qualities.parquet\"\n",
    "df_export.to_parquet(save_path, index=False)\n",
    "print(f\"\\nExported: {save_path}\")\n",
    "print(f\"Shape: {df_export.shape}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(df_export.groupby([\"from_position\", \"cluster_name\"]).size().reset_index(name=\"count\").to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}