{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 â€” Prepare Player Data for Sub-Role Clustering\n",
    "\n",
    "This notebook loads the transfer dataset, de-duplicates player-seasons, and prepares two feature sets for clustering:\n",
    "\n",
    "- **Path A**: 20 Player Qualities (pre-computed by Twelve Football)\n",
    "- **Path B**: 75 Per-90 Z-Score columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Dynamic path resolution\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for d in docs.iterdir():\n",
    "    if \"Jorge\" in d.name and \"MacBook\" in d.name and d.is_dir():\n",
    "        BASE = d / \"thesis_data\" / \"raw_data\"\n",
    "        NB_DIR = d / \"thesis_data\" / \"notebooks\" / \"player_subroles\"\n",
    "        break\n",
    "\n",
    "NB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the main transfer dataset\n",
    "transfer_path = BASE / \"Transfers\" / \"transfers_model_v2_2018_2025.parquet\"\n",
    "df = pd.read_parquet(transfer_path)\n",
    "print(f\"Loaded dataset: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"\\nPosition distribution (from_position):\")\n",
    "print(df[\"from_position\"].value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. De-duplicate Player-Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique transfer rows vs unique player-team-season combinations\n",
    "n_total = len(df)\n",
    "n_unique_combos = df.groupby([\"wy_player_id\", \"from_team_id\", \"from_season\"]).ngroups\n",
    "print(f\"Total transfer rows:                   {n_total:,}\")\n",
    "print(f\"Unique (player, team, season) combos:  {n_unique_combos:,}\")\n",
    "print(f\"Duplicate rows to drop:                {n_total - n_unique_combos:,}\")\n",
    "print()\n",
    "\n",
    "# Before de-duplication counts per position\n",
    "print(\"--- BEFORE de-duplication ---\")\n",
    "before_counts = df[\"from_position\"].value_counts().sort_values(ascending=False)\n",
    "print(before_counts)\n",
    "print()\n",
    "\n",
    "# De-duplicate: keep first occurrence per (wy_player_id, from_team_id, from_season)\n",
    "df_dedup = df.drop_duplicates(subset=[\"wy_player_id\", \"from_team_id\", \"from_season\"], keep=\"first\").copy()\n",
    "print(\"--- AFTER de-duplication ---\")\n",
    "after_counts = df_dedup[\"from_position\"].value_counts().sort_values(ascending=False)\n",
    "print(after_counts)\n",
    "print(f\"\\nTotal after de-duplication: {len(df_dedup):,}\")\n",
    "\n",
    "# Filter to minimum 500 minutes\n",
    "df_dedup = df_dedup[df_dedup[\"from_Minutes\"] >= 500].copy()\n",
    "print(f\"\\n--- AFTER >= 500 minutes filter ---\")\n",
    "final_counts = df_dedup[\"from_position\"].value_counts().sort_values(ascending=False)\n",
    "print(final_counts)\n",
    "print(f\"\\nTotal after minutes filter: {len(df_dedup):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Path A: 20 Player Qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The 20 player qualities\nall_qualities = [\n    \"Active defence\", \"Aerial threat\", \"Box threat\", \"Chance prevention\",\n    \"Composure\", \"Defensive heading\", \"Dribbling\", \"Effectiveness\",\n    \"Finishing\", \"Hold-up play\", \"Intelligent defence\", \"Involvement\",\n    \"Passing quality\", \"Poaching\", \"Pressing\", \"Progression\",\n    \"Providing teammates\", \"Run quality\", \"Territorial dominance\", \"Winning duels\"\n]\n\nquality_cols = [f\"from_{q}\" for q in all_qualities]\n\n# Check which quality columns actually exist in the data\nexisting_quality_cols = [c for c in quality_cols if c in df_dedup.columns]\nmissing_quality_cols = [c for c in quality_cols if c not in df_dedup.columns]\nprint(f\"Quality columns found: {len(existing_quality_cols)} / {len(quality_cols)}\")\nif missing_quality_cols:\n    print(f\"Missing: {missing_quality_cols}\")\n\n# Define position-specific quality expectations\n# Universal qualities (expected for all outfield positions)\nuniversal_qualities = [\n    \"Active defence\", \"Aerial threat\", \"Box threat\", \"Composure\",\n    \"Defensive heading\", \"Dribbling\", \"Effectiveness\", \"Finishing\",\n    \"Hold-up play\", \"Intelligent defence\", \"Involvement\", \"Passing quality\",\n    \"Pressing\", \"Progression\", \"Providing teammates\", \"Run quality\", \"Winning duels\"\n]\n\n# Position-specific additions\ncb_fb_extras = [\"Chance prevention\", \"Territorial dominance\"]\nstriker_winger_extras = [\"Poaching\"]\n\npositions = [\"Central Defender\", \"Full Back\", \"Midfielder\", \"Winger\", \"Striker\", \"Goalkeeper\"]\n\n# Determine coverage (>80% non-null) per position per quality\nprint(\"\\n--- Coverage Analysis (% non-null per position per quality) ---\")\ncoverage_threshold = 0.80\n\nposition_quality_features = {}\nfor pos in positions:\n    pos_data = df_dedup[df_dedup[\"from_position\"] == pos]\n    n_pos = len(pos_data)\n    if n_pos == 0:\n        continue\n    \n    # Check coverage for each quality\n    valid_features = []\n    print(f\"\\n{pos} (n={n_pos:,}):\")\n    for q in all_qualities:\n        col = f\"from_{q}\"\n        if col not in df_dedup.columns:\n            continue\n        coverage = pos_data[col].notna().mean()\n        marker = \"YES\" if coverage >= coverage_threshold else \"no\"\n        print(f\"  {q:30s} coverage={coverage:.1%}  [{marker}]\")\n        if coverage >= coverage_threshold:\n            valid_features.append(col)\n    \n    position_quality_features[pos] = valid_features\n    print(f\"  => {len(valid_features)} features pass the 80% coverage threshold\")\n\n# Build output: keep all relevant columns per player\n# Include identifiers + all quality columns (even if NaN for some positions)\nid_cols = [\"wy_player_id\", \"from_team_id\", \"from_season\", \"from_position\", \"from_Minutes\"]\n# Also include player name if available (short_name is the actual column in our dataset)\nname_candidates = [\"short_name\", \"from_short_name\", \"from_player_name\", \"player_name\"]\nfor nc in name_candidates:\n    if nc in df_dedup.columns:\n        id_cols.append(nc)\n        break\n\nout_cols = id_cols + existing_quality_cols\ndf_qualities = df_dedup[out_cols].copy()\n\n# Save\nout_path = NB_DIR / \"player_data_qualities.parquet\"\ndf_qualities.to_parquet(out_path, index=False)\nprint(f\"\\nSaved: {out_path}\")\nprint(f\"Shape: {df_qualities.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Path B: 75 Per-90 Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract all z-score columns\nzscore_cols = [c for c in df_dedup.columns if c.startswith(\"from_z_score_\")]\nprint(f\"Total z-score columns found: {len(zscore_cols)}\")\n\n# Check coverage per position\nprint(\"\\n--- Z-Score Coverage Analysis ---\")\n\nposition_zscore_features = {}\nfor pos in positions:\n    pos_data = df_dedup[df_dedup[\"from_position\"] == pos]\n    n_pos = len(pos_data)\n    if n_pos == 0:\n        continue\n    \n    # Coverage for each z-score column\n    coverages = pos_data[zscore_cols].notna().mean()\n    valid_zscores = coverages[coverages >= coverage_threshold].index.tolist()\n    low_coverage = coverages[coverages < coverage_threshold]\n    \n    position_zscore_features[pos] = valid_zscores\n    print(f\"\\n{pos} (n={n_pos:,}):\")\n    print(f\"  z-score features with >80% coverage: {len(valid_zscores)} / {len(zscore_cols)}\")\n    if len(low_coverage) > 0:\n        print(f\"  Dropped (low coverage): {len(low_coverage)} columns\")\n        for col_name, cov in low_coverage.sort_values().head(5).items():\n            short_name_col = col_name.replace(\"from_z_score_\", \"\")\n            print(f\"    {short_name_col:50s} coverage={cov:.1%}\")\n        if len(low_coverage) > 5:\n            print(f\"    ... and {len(low_coverage) - 5} more\")\n\n# Build output\nid_cols_z = [\"wy_player_id\", \"from_team_id\", \"from_season\", \"from_position\", \"from_Minutes\"]\nname_candidates = [\"short_name\", \"from_short_name\", \"from_player_name\", \"player_name\"]\nfor nc in name_candidates:\n    if nc in df_dedup.columns:\n        id_cols_z.append(nc)\n        break\n\nout_cols_z = id_cols_z + zscore_cols\ndf_zscores = df_dedup[out_cols_z].copy()\n\n# Save\nout_path_z = NB_DIR / \"player_data_zscores.parquet\"\ndf_zscores.to_parquet(out_path_z, index=False)\nprint(f\"\\nSaved: {out_path_z}\")\nprint(f\"Shape: {df_zscores.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_rows = []\n",
    "for pos in positions:\n",
    "    n_players = len(df_dedup[df_dedup[\"from_position\"] == pos])\n",
    "    n_qual = len(position_quality_features.get(pos, []))\n",
    "    n_zsc = len(position_zscore_features.get(pos, []))\n",
    "    summary_rows.append({\n",
    "        \"Position\": pos,\n",
    "        \"N Players (>=500 min)\": n_players,\n",
    "        \"Quality Features (Path A)\": n_qual,\n",
    "        \"Z-Score Features (Path B)\": n_zsc,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"=\" * 75)\n",
    "print(\"SUMMARY: Features Available per Position\")\n",
    "print(\"=\" * 75)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\nFiles saved:\")\n",
    "print(f\"  - {NB_DIR / 'player_data_qualities.parquet'}\")\n",
    "print(f\"  - {NB_DIR / 'player_data_zscores.parquet'}\")\n",
    "\n",
    "# Save the feature mappings for downstream notebooks\n",
    "import json\n",
    "\n",
    "feature_map = {\n",
    "    \"position_quality_features\": {pos: cols for pos, cols in position_quality_features.items()},\n",
    "    \"position_zscore_features\": {pos: cols for pos, cols in position_zscore_features.items()},\n",
    "}\n",
    "with open(NB_DIR / \"feature_maps.json\", \"w\") as f:\n",
    "    json.dump(feature_map, f, indent=2)\n",
    "print(f\"  - {NB_DIR / 'feature_maps.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}