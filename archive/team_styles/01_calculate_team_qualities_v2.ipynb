{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Calculate Team Style Qualities\n",
    "\n",
    "Implements the supervisor's methodology: for each team-season, calculate **6 style qualities + 1 outcome quality** as weighted averages of z-scores computed **within each competition-season**.\n",
    "\n",
    "Source: `teams_qualities.md` (Twelve Football internal methodology)\n",
    "\n",
    "**Output:** `team_qualities.parquet` — one row per (team, competition, season) with 7 quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Dynamic path resolution (handles Unicode apostrophe)\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for d in docs.iterdir():\n",
    "    if \"Jorge\" in d.name and \"MacBook\" in d.name and d.is_dir():\n",
    "        BASE = d / \"thesis_data\" / \"raw_data\"\n",
    "        break\n",
    "\n",
    "ts = pd.read_parquet(BASE / \"Teams_stats\" / \"team_stats_season.parquet\")\n",
    "print(f\"Loaded: {ts.shape[0]:,} team-seasons  |  {ts.team_id.nunique():,} teams  |  {ts.competition_id.nunique()} competitions  |  seasons {ts.season.min()}-{ts.season.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Quality Formulas\n",
    "\n",
    "Each quality is a weighted average of z-scores. Metrics where **lower is better** get their z-scores negated before weighting (so that a positive quality score always means \"more of that style\").\n",
    "\n",
    "Quality directions:\n",
    "- **DEFENCE:** low = compact low block → high = aggressive high press\n",
    "- **DEFENSIVE TRANSITION:** low = drop back → high = counter-press\n",
    "- **ATTACKING TRANSITION:** low = retain & build up → high = quick counter-attack\n",
    "- **ATTACK:** low = patient build-up → high = direct long balls\n",
    "- **PENETRATION:** low = wide crossing → high = progressive carrying\n",
    "- **CHANCE CREATION:** low = sustained possession → high = direct/fast chances\n",
    "- **OUTCOME:** low = poor results → high = strong results (separate from style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher_is_better: True means a high raw value pushes the quality HIGHER\n",
    "#                    False means the z-score gets negated (lower raw = higher quality)\n",
    "\n",
    "QUALITIES = {\n",
    "    \"defence\": {\n",
    "        \"description\": \"Low block ← → High press\",\n",
    "        \"metrics\": {\n",
    "            \"defensive_intensity\":       {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "            \"ppda\":                      {\"weight\": 1.0, \"higher_is_better\": False},  # lower ppda = more pressing\n",
    "            \"final_third_recoveries_pct\": {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "            \"defensive_action_height_m\": {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "        },\n",
    "    },\n",
    "    \"defensive_transition\": {\n",
    "        \"description\": \"Drop back ← → Counter-press\",\n",
    "        \"metrics\": {\n",
    "            \"recoveries_within_5s_pct\":                          {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "            \"time_to_defensive_action_after_loss_att_half_s\":    {\"weight\": 2.0, \"higher_is_better\": False},  # lower = faster\n",
    "            \"time_to_defensive_action_after_loss_own_half_s\":    {\"weight\": 1.0, \"higher_is_better\": False},\n",
    "        },\n",
    "    },\n",
    "    \"attacking_transition\": {\n",
    "        \"description\": \"Retain & build up ← → Quick counter-attack\",\n",
    "        \"metrics\": {\n",
    "            \"possessions_retained_after_5s_pct\":                          {\"weight\": 0.5, \"higher_is_better\": False},  # high retention = NOT counter\n",
    "            \"final_third_entry_within_10s_after_recovery_own_half_pct\":   {\"weight\": 0.5, \"higher_is_better\": True},\n",
    "            \"first_pass_forward_after_recovery_own_half_pct\":             {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "            \"median_time_to_first_forward_pass_own_half_s\":               {\"weight\": 0.5, \"higher_is_better\": False},  # lower = faster\n",
    "        },\n",
    "    },\n",
    "    \"attack\": {\n",
    "        \"description\": \"Patient build-up ← → Direct long balls\",\n",
    "        \"metrics\": {\n",
    "            \"long_ball_pct\":                       {\"weight\": 2.0, \"higher_is_better\": True},\n",
    "            \"forward_passes_from_middle_third_pct\": {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "            \"buildups_from_goalkicks_pct\":          {\"weight\": 1.0, \"higher_is_better\": False},  # high = build from back = NOT direct\n",
    "        },\n",
    "    },\n",
    "    \"penetration\": {\n",
    "        \"description\": \"Wide crossing ← → Progressive carrying\",\n",
    "        \"metrics\": {\n",
    "            \"box_entries_from_carries_pct\":      {\"weight\": 2.0, \"higher_is_better\": True},\n",
    "            \"box_entries_from_crosses_pct\":      {\"weight\": 2.0, \"higher_is_better\": False},  # high crosses = NOT carrying\n",
    "            \"crosses_per_final_third_possession\": {\"weight\": 1.0, \"higher_is_better\": False},\n",
    "        },\n",
    "    },\n",
    "    \"chance_creation\": {\n",
    "        \"description\": \"Sustained possession ← → Direct fast chances\",\n",
    "        \"metrics\": {\n",
    "            \"shots_per_final_third_pass\":      {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "            \"shots_from_direct_attacks_pct\":   {\"weight\": 2.0, \"higher_is_better\": True},\n",
    "            \"shots_from_sustained_attacks_pct\": {\"weight\": 2.0, \"higher_is_better\": False},  # high sustained = NOT direct\n",
    "        },\n",
    "    },\n",
    "    \"outcome\": {\n",
    "        \"description\": \"Poor results ← → Strong results (not a style)\",\n",
    "        \"metrics\": {\n",
    "            \"xpts\":   {\"weight\": 1.5, \"higher_is_better\": True},\n",
    "            \"points\": {\"weight\": 1.0, \"higher_is_better\": True},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Verify all metrics exist\n",
    "all_metrics = set()\n",
    "for q, qdef in QUALITIES.items():\n",
    "    for m in qdef[\"metrics\"]:\n",
    "        all_metrics.add(m)\n",
    "        assert m in ts.columns, f\"Missing column: {m}\"\n",
    "\n",
    "print(f\"All {len(all_metrics)} metrics verified in dataset.\")\n",
    "for q, qdef in QUALITIES.items():\n",
    "    metrics = list(qdef['metrics'].keys())\n",
    "    weights = [qdef['metrics'][m]['weight'] for m in metrics]\n",
    "    print(f\"  {q:25s} — {len(metrics)} metrics, total weight {sum(weights):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Z-Scores Within Competition-Season\n",
    "\n",
    "For each metric, standardize within `(competition_id, season)` so that z-scores reflect how a team compares **to others in the same league and season**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check minimum teams per competition-season (need ≥3 for meaningful z-scores)\n",
    "group_sizes = ts.groupby([\"competition_id\", \"season\"]).size()\n",
    "print(f\"Competition-seasons: {len(group_sizes):,}\")\n",
    "print(f\"Teams per group: min={group_sizes.min()}, median={group_sizes.median():.0f}, max={group_sizes.max()}\")\n",
    "print(f\"Groups with <3 teams: {(group_sizes < 3).sum()} ({(group_sizes < 3).mean()*100:.1f}%)\")\n",
    "print(f\"Groups with <5 teams: {(group_sizes < 5).sum()} ({(group_sizes < 5).mean()*100:.1f}%)\")\n",
    "\n",
    "# Filter: require at least 3 teams in a competition-season for z-scores to be meaningful\n",
    "MIN_TEAMS = 3\n",
    "valid_groups = group_sizes[group_sizes >= MIN_TEAMS].index\n",
    "ts_filtered = ts.set_index([\"competition_id\", \"season\"]).loc[valid_groups].reset_index()\n",
    "print(f\"\\nAfter filtering (≥{MIN_TEAMS} teams): {len(ts_filtered):,} rows ({len(ts_filtered)/len(ts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores within each (competition_id, season)\n",
    "metrics_list = sorted(all_metrics)\n",
    "\n",
    "def zscore_within_group(group, cols):\n",
    "    \"\"\"Z-score each metric within a competition-season group.\"\"\"\n",
    "    result = group[cols].copy()\n",
    "    for c in cols:\n",
    "        mean = result[c].mean()\n",
    "        std = result[c].std()\n",
    "        if std > 0:\n",
    "            result[c] = (result[c] - mean) / std\n",
    "        else:\n",
    "            result[c] = 0.0  # no variation in this group\n",
    "    return result\n",
    "\n",
    "# Apply groupwise z-scoring\n",
    "z_scores = ts_filtered.groupby([\"competition_id\", \"season\"], group_keys=False).apply(\n",
    "    zscore_within_group, cols=metrics_list\n",
    ")\n",
    "\n",
    "# Attach back to keys\n",
    "z_df = ts_filtered[[\"team_id\", \"competition_id\", \"season\"]].copy()\n",
    "for c in metrics_list:\n",
    "    z_df[f\"z_{c}\"] = z_scores[c].values\n",
    "\n",
    "print(f\"Z-scores computed: {z_df.shape}\")\n",
    "print(f\"\\nSample z-score distributions (should be ~mean=0, std=1 within each group):\")\n",
    "sample_metrics = [\"defensive_intensity\", \"ppda\", \"ball_possession_pct\"]\n",
    "for m in sample_metrics:\n",
    "    vals = z_df[f\"z_{m}\"]\n",
    "    print(f\"  z_{m}: mean={vals.mean():.4f}, std={vals.std():.3f}, range=[{vals.min():.2f}, {vals.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Weighted Quality Scores\n",
    "\n",
    "For each quality: negate z-scores of \"lower is better\" metrics, then compute weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_df = z_df[[\"team_id\", \"competition_id\", \"season\"]].copy()\n",
    "\n",
    "for q_name, q_def in QUALITIES.items():\n",
    "    weighted_sum = np.zeros(len(z_df))\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    for metric, mdef in q_def[\"metrics\"].items():\n",
    "        w = mdef[\"weight\"]\n",
    "        z_col = z_df[f\"z_{metric}\"].values\n",
    "        \n",
    "        # Negate if lower raw value is \"better\" for this quality direction\n",
    "        if not mdef[\"higher_is_better\"]:\n",
    "            z_col = -z_col\n",
    "        \n",
    "        weighted_sum += w * z_col\n",
    "        total_weight += w\n",
    "    \n",
    "    quality_df[q_name] = weighted_sum / total_weight\n",
    "\n",
    "style_cols = [c for c in quality_df.columns if c not in [\"team_id\", \"competition_id\", \"season\"]]\n",
    "print(f\"Computed {len(style_cols)} qualities for {len(quality_df):,} team-seasons:\")\n",
    "for c in style_cols:\n",
    "    vals = quality_df[c]\n",
    "    print(f\"  {c:25s}  mean={vals.mean():+.3f}  std={vals.std():.3f}  range=[{vals.min():.2f}, {vals.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: quality distributions should be roughly centered around 0\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, q in enumerate(style_cols):\n",
    "    ax = axes[i]\n",
    "    ax.hist(quality_df[q].dropna(), bins=60, color=\"#1565C0\", alpha=0.8, edgecolor=\"white\")\n",
    "    ax.set_title(q.replace(\"_\", \" \").title(), fontsize=13, fontweight=\"bold\")\n",
    "    ax.axvline(0, color=\"red\", lw=1, ls=\"--\")\n",
    "    ax.set_xlabel(\"Quality score\")\n",
    "\n",
    "# Hide extra subplot\n",
    "if len(style_cols) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Team Quality Score Distributions (z-scored within competition-season)\", fontsize=16, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(BASE.parent / \"notebooks\" / \"team_styles\" / \"quality_distributions.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: quality_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full quality dataset\n",
    "out_path = BASE / \"Teams_stats\" / \"team_qualities.parquet\"\n",
    "quality_df.to_parquet(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "print(f\"Shape: {quality_df.shape}\")\n",
    "print(f\"Columns: {list(quality_df.columns)}\")\n",
    "print(f\"\\nSample (first 10 rows):\")\n",
    "quality_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEAM QUALITIES SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total team-seasons: {len(quality_df):,}\")\n",
    "print(f\"Unique teams: {quality_df.team_id.nunique():,}\")\n",
    "print(f\"Unique competitions: {quality_df.competition_id.nunique()}\")\n",
    "print(f\"Season range: {quality_df.season.min()}-{quality_df.season.max()}\")\n",
    "print(f\"\\nStyle qualities (6): {[c for c in style_cols if c != 'outcome']}\")\n",
    "print(f\"Outcome quality (1): outcome\")\n",
    "print(f\"\\nNull check:\")\n",
    "for c in style_cols:\n",
    "    n_null = quality_df[c].isnull().sum()\n",
    "    if n_null > 0:\n",
    "        print(f\"  {c}: {n_null} nulls ({n_null/len(quality_df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {c}: 0 nulls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}