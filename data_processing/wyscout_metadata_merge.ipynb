{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriquecimiento con metadata de Wyscout\n",
    "\n",
    "Dos merges sobre `male_transfers_model_2018_2025.parquet`:\n",
    "\n",
    "1. **Players** (`players_wyscout.parquet`): left join por `player_id` → 9 columnas `wyscout_*`\n",
    "2. **Competitions** (`competitions_wyscout.parquet`): left join por `(competition, season)` × 2 (from + to) → 8 `from_comp_*` + 8 `to_comp_*`\n",
    "\n",
    "Resultado final: **262,340 rows × 385 cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Paths (resolve Unicode dir name dynamically) --\n",
    "from pathlib import Path\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for _d in docs.iterdir():\n",
    "    if \"Jorge\" in _d.name and \"MacBook\" in _d.name and _d.is_dir():\n",
    "        RAW = _d / \"thesis_data\" / \"raw_data\"\n",
    "        PROCESSED = _d / \"thesis_data\" / \"processed_data\"\n",
    "        break\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "ws = pd.read_parquet(base / \"Wyscout\" / \"players_wyscout.parquet\")\n",
    "wc = pd.read_parquet(base / \"Wyscout\" / \"competitions_wyscout.parquet\")\n",
    "tf = pd.read_parquet(base / \"Transfers\" / \"male_transfers_model_2018_2025.parquet\")\n",
    "\n",
    "# Drop comp metadata columns if they already exist (re-run safe)\n",
    "old_comp = [c for c in tf.columns if c.startswith(\"from_comp_\") or c.startswith(\"to_comp_\")]\n",
    "if old_comp:\n",
    "    tf = tf.drop(columns=old_comp)\n",
    "    print(f\"Dropped {len(old_comp)} existing comp metadata cols\")\n",
    "\n",
    "print(f\"Transfers:    {tf.shape}\")\n",
    "print(f\"Players WS:   {ws.shape}\")\n",
    "print(f\"Comps WS:     {wc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 1: Wyscout Players metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"players_wyscout columns ({len(ws.columns)}):\")\n",
    "for c in ws.columns:\n",
    "    print(f\"  {c:20s} dtype={str(ws[c].dtype):10s} nulls={ws[c].isna().sum():>6,}  nunique={ws[c].nunique():>6,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cobertura por player_id\n",
    "tf_ids = set(tf[\"player_id\"].unique())\n",
    "ws_ids = set(ws[\"player_id\"].unique())\n",
    "overlap = tf_ids & ws_ids\n",
    "\n",
    "print(f\"Jugadores únicos en transfers: {len(tf_ids):,}\")\n",
    "print(f\"Jugadores únicos en wyscout:   {len(ws_ids):,}\")\n",
    "print(f\"Overlap:                       {len(overlap):,} ({len(overlap)/len(tf_ids)*100:.1f}%)\")\n",
    "\n",
    "rows_match = tf[\"player_id\"].isin(ws_ids).sum()\n",
    "print(f\"\\nRows con match:  {rows_match:>7,} / {len(tf):,} ({rows_match/len(tf)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge player metadata\n",
    "ws_target = [\"player_id\", \"first_name\", \"last_name\", \"height\", \"weight\",\n",
    "             \"passport\", \"birth_country\", \"image_url\", \"foot\", \"role\"]\n",
    "ws_meta = ws[ws_target].rename(columns={c: f\"wyscout_{c}\" for c in ws_target if c != \"player_id\"})\n",
    "\n",
    "df = tf.merge(ws_meta, on=\"player_id\", how=\"left\")\n",
    "assert len(df) == len(tf)\n",
    "print(f\"Shape: {df.shape} (added {len(df.columns) - len(tf.columns)} cols)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Columna':<35s} {'Filled':>7s} / {'Total':>7s}  {'%':>5s}\")\n",
    "print(\"-\" * 60)\n",
    "for c in [col for col in df.columns if col.startswith(\"wyscout_\")]:\n",
    "    n = df[c].notna().sum()\n",
    "    print(f\"{c:<35s} {n:>7,} / {len(df):>7,}  {n/len(df)*100:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Wyscout Competitions metadata\n",
    "\n",
    "Cada competición tiene múltiples seasons. Join key compuesto: `(competition_id, season)`.\n",
    "\n",
    "Se aplica dos veces:\n",
    "- `from_competition` + `from_season` → columnas `from_comp_*`\n",
    "- `to_competition` + `to_season` → columnas `to_comp_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"competitions_wyscout: {wc.shape}\")\n",
    "print(f\"\\nGranularity: {wc.groupby(['competition_id','season']).ngroups} unique (comp_id, season) from {len(wc)} rows\")\n",
    "n_dupes = wc.duplicated(subset=['competition_id','season']).sum()\n",
    "print(f\"Duplicates on (comp_id, season): {n_dupes} → {n_dupes} combos con >1 registro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Análisis de los 42 duplicados (competition_id, season)\n",
    "\n",
    "Se agrupan en 3 patrones:\n",
    "\n",
    "| Patrón | Casos | Descripción |\n",
    "|--------|-------|-------------|\n",
    "| NCAA Spring/Fall | 29 | Ligas universitarias USA con 2 mitades por año |\n",
    "| COVID suspended | 8 | Seasons 2020 suspendidas + reapertas (Australia, China, Venezuela, Brazil, Zimbabwe) |\n",
    "| Calendarios split | 5 | Ligas con calendarios año natural partido (Angola, Ghana, India, Nicaragua, Rep. Dominicana, Vietnam, Grecia) |\n",
    "\n",
    "**Impacto:** Solo 1,938 rows from (0.7%) y 1,301 rows to (0.5%) del dataset de transfers caen en estas combos.\n",
    "\n",
    "**Criterio de deduplicación:**\n",
    "- Deprioritizar registros cuyo `season_name` contenga \"suspended\", \"cancelled\" o \"spring\"\n",
    "- De los restantes, quedarse con el de `end_date` más tardía (season más completa / Fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar todos los duplicados\n",
    "dupes = wc[wc.duplicated(subset=[\"competition_id\", \"season\"], keep=False)].sort_values([\"country\", \"name\", \"season\"])\n",
    "print(f\"Total rows con duplicado: {len(dupes)} ({dupes.groupby(['competition_id','season']).ngroups} combos)\\n\")\n",
    "\n",
    "for (cid, s), g in dupes.groupby([\"competition_id\", \"season\"]):\n",
    "    r0 = g.iloc[0]\n",
    "    print(f\"--- {r0['country']} | {r0['name']} | season={s} ---\")\n",
    "    for _, r in g.iterrows():\n",
    "        print(f\"  season_id={r['season_id']:<8}  {r['start_date']} → {r['end_date']}  \\\"{r['season_name']}\\\"  completed={r['completed']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impacto en transfers\n",
    "dupe_keys = set(dupes.groupby([\"competition_id\", \"season\"]).groups.keys())\n",
    "dupe_df = pd.DataFrame(list(dupe_keys), columns=[\"comp\", \"season\"])\n",
    "\n",
    "from_hits = tf.merge(dupe_df, left_on=[\"from_competition\", \"from_season\"], right_on=[\"comp\", \"season\"], how=\"inner\")\n",
    "to_hits = tf.merge(dupe_df, left_on=[\"to_competition\", \"to_season\"], right_on=[\"comp\", \"season\"], how=\"inner\")\n",
    "\n",
    "print(f\"Transfer rows afectadas en FROM: {len(from_hits):,} / {len(tf):,} ({len(from_hits)/len(tf)*100:.2f}%)\")\n",
    "print(f\"Transfer rows afectadas en TO:   {len(to_hits):,} / {len(tf):,} ({len(to_hits)/len(tf)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicar\n",
    "comp_target = [\"competition_id\", \"season\", \"name\", \"country\", \"division\",\n",
    "               \"season_id\", \"start_date\", \"end_date\", \"completed\", \"season_name\"]\n",
    "wc_meta = wc[comp_target].copy()\n",
    "wc_meta[\"start_date\"] = pd.to_datetime(wc_meta[\"start_date\"])\n",
    "wc_meta[\"end_date\"] = pd.to_datetime(wc_meta[\"end_date\"])\n",
    "\n",
    "# Deprioritize suspended/cancelled/spring\n",
    "drop_keywords = [\"suspended\", \"cancelled\", \"spring\"]\n",
    "wc_meta[\"_deprioritize\"] = wc_meta[\"season_name\"].str.lower().apply(\n",
    "    lambda x: any(k in x for k in drop_keywords)\n",
    ")\n",
    "wc_meta = wc_meta.sort_values([\"_deprioritize\", \"end_date\"], ascending=[True, False])\n",
    "wc_meta = wc_meta.drop_duplicates(subset=[\"competition_id\", \"season\"], keep=\"first\")\n",
    "wc_meta = wc_meta.drop(columns=[\"_deprioritize\"])\n",
    "\n",
    "print(f\"Deduped: {len(wc_meta)} rows (dropped {len(wc) - len(wc_meta)} duplicates)\")\n",
    "assert wc_meta.duplicated(subset=[\"competition_id\", \"season\"]).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que los elegidos son los correctos (no suspended/cancelled/spring)\n",
    "kept = wc_meta[wc_meta[\"competition_id\"].isin(dupes[\"competition_id\"])]\n",
    "kept_in_dupes = kept.merge(dupes[[\"competition_id\",\"season\"]].drop_duplicates(), on=[\"competition_id\",\"season\"])\n",
    "print(\"Registros elegidos para las 42 combos duplicadas:\")\n",
    "print(kept_in_dupes[[\"competition_id\", \"season\", \"name\", \"country\", \"season_name\", \"start_date\", \"end_date\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge competitions (FROM + TO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM join\n",
    "from_meta = wc_meta.rename(columns={\n",
    "    c: f\"from_comp_{c}\" for c in comp_target if c not in [\"competition_id\", \"season\"]\n",
    "})\n",
    "from_meta = from_meta.rename(columns={\"competition_id\": \"from_competition\", \"season\": \"from_season\"})\n",
    "from_meta[\"from_competition\"] = from_meta[\"from_competition\"].astype(\"int32\")\n",
    "from_meta[\"from_season\"] = from_meta[\"from_season\"].astype(\"int16\")\n",
    "\n",
    "df = df.merge(from_meta, on=[\"from_competition\", \"from_season\"], how=\"left\")\n",
    "print(f\"After FROM comp merge: {df.shape}\")\n",
    "\n",
    "# TO join\n",
    "to_meta = wc_meta.rename(columns={\n",
    "    c: f\"to_comp_{c}\" for c in comp_target if c not in [\"competition_id\", \"season\"]\n",
    "})\n",
    "to_meta = to_meta.rename(columns={\"competition_id\": \"to_competition\", \"season\": \"to_season\"})\n",
    "to_meta[\"to_competition\"] = to_meta[\"to_competition\"].astype(\"int32\")\n",
    "to_meta[\"to_season\"] = to_meta[\"to_season\"].astype(\"int16\")\n",
    "\n",
    "df = df.merge(to_meta, on=[\"to_competition\", \"to_season\"], how=\"left\")\n",
    "print(f\"After TO comp merge:   {df.shape}\")\n",
    "assert len(df) == len(tf), \"Row count changed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cobertura\n",
    "print(\"=== FROM competition metadata ===\")\n",
    "for c in sorted([c for c in df.columns if c.startswith(\"from_comp_\")]):\n",
    "    n = df[c].notna().sum()\n",
    "    print(f\"  {c:30s} {n:>7,} / {len(df):,} ({n/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== TO competition metadata ===\")\n",
    "for c in sorted([c for c in df.columns if c.startswith(\"to_comp_\")]):\n",
    "    n = df[c].notna().sum()\n",
    "    print(f\"  {c:30s} {n:>7,} / {len(df):,} ({n/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Reordenar y guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(c):\n",
    "    s = c.split(\"_\", 1)[1]\n",
    "    if s in [\"team_id\", \"competition\", \"season\", \"position\", \"Minutes\"]:\n",
    "        return \"meta\"\n",
    "    if s.startswith(\"z_score_\"):\n",
    "        return \"zscore\"\n",
    "    if \"per 90\" in s:\n",
    "        return \"per90\"\n",
    "    return \"raw\"\n",
    "\n",
    "player_meta = [\"player_id\", \"short_name\", \"birth_date\", \"player_season_age\", \"transfer_type\"]\n",
    "wyscout_player = sorted([c for c in df.columns if c.startswith(\"wyscout_\")])\n",
    "transfer_dates = [\"competition_start_date\", \"first_played_date\", \"last_played_date\"]\n",
    "\n",
    "from_orig = [c for c in tf.columns if c.startswith(\"from_\")]\n",
    "to_orig = [c for c in tf.columns if c.startswith(\"to_\")]\n",
    "from_comp = sorted([c for c in df.columns if c.startswith(\"from_comp_\")])\n",
    "to_comp = sorted([c for c in df.columns if c.startswith(\"to_comp_\")])\n",
    "\n",
    "from_by_cat = {\"meta\": [], \"raw\": [], \"per90\": [], \"zscore\": []}\n",
    "for c in from_orig:\n",
    "    from_by_cat[classify(c)].append(c)\n",
    "to_by_cat = {\"meta\": [], \"raw\": [], \"per90\": [], \"zscore\": []}\n",
    "for c in to_orig:\n",
    "    to_by_cat[classify(c)].append(c)\n",
    "\n",
    "ordered = (\n",
    "    player_meta + wyscout_player + transfer_dates +\n",
    "    from_by_cat[\"meta\"] + from_comp + from_by_cat[\"raw\"] + from_by_cat[\"per90\"] + from_by_cat[\"zscore\"] +\n",
    "    to_by_cat[\"meta\"] + to_comp + to_by_cat[\"raw\"] + to_by_cat[\"per90\"] + to_by_cat[\"zscore\"]\n",
    ")\n",
    "\n",
    "assert set(ordered) == set(df.columns) and len(ordered) == len(df.columns)\n",
    "df_final = df[ordered]\n",
    "print(f\"Final: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [\n",
    "    (\"Player metadata\", player_meta),\n",
    "    (\"Wyscout player\", wyscout_player),\n",
    "    (\"Transfer dates\", transfer_dates),\n",
    "    (\"FROM metadata\", from_by_cat[\"meta\"]),\n",
    "    (\"FROM comp metadata\", from_comp),\n",
    "    (\"FROM raw metrics\", from_by_cat[\"raw\"]),\n",
    "    (\"FROM per 90\", from_by_cat[\"per90\"]),\n",
    "    (\"FROM z-scores\", from_by_cat[\"zscore\"]),\n",
    "    (\"TO metadata\", to_by_cat[\"meta\"]),\n",
    "    (\"TO comp metadata\", to_comp),\n",
    "    (\"TO raw metrics\", to_by_cat[\"raw\"]),\n",
    "    (\"TO per 90\", to_by_cat[\"per90\"]),\n",
    "    (\"TO z-scores\", to_by_cat[\"zscore\"]),\n",
    "]\n",
    "\n",
    "idx = 0\n",
    "print(f\"{'Bloque':<25s} {'Rango':<14s} {'#':>4s}\")\n",
    "print(\"-\" * 45)\n",
    "for name, cols in blocks:\n",
    "    print(f\"{name:<25s} [{idx:>3d} - {idx+len(cols)-1:>3d}]  {len(cols):>4d}\")\n",
    "    idx += len(cols)\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'TOTAL':<25s} {'':14s} {idx:>4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar espejo from_comp ↔ to_comp\n",
    "from_comp_suf = sorted([c.replace(\"from_comp_\", \"\") for c in from_comp])\n",
    "to_comp_suf = sorted([c.replace(\"to_comp_\", \"\") for c in to_comp])\n",
    "print(f\"from_comp_ == to_comp_ suffixes: {from_comp_suf == to_comp_suf}\")\n",
    "print(f\"Columns: {from_comp_suf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = base / \"Transfers\" / \"male_transfers_model_2018_2025.parquet\"\n",
    "df_final.to_parquet(out_path, index=False)\n",
    "\n",
    "print(f\"Guardado: {out_path}\")\n",
    "print(f\"Shape:    {df_final.shape}\")\n",
    "print(f\"Tamaño:   {out_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "df_check = pd.read_parquet(out_path)\n",
    "assert list(df_check.columns) == ordered and df_check.shape == df_final.shape\n",
    "print(f\"\\n✅ Verificado: {df_check.shape[0]:,} rows × {df_check.shape[1]} cols\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}