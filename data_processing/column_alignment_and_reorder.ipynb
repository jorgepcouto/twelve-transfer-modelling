{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alineaci√≥n y reordenamiento de columnas ‚Äî Transfers Dataset\n",
    "\n",
    "Partimos de `male_transfers_model_2018_2025.parquet` (262,340 rows).\n",
    "\n",
    "**Objetivos:**\n",
    "1. Verificar que las 176 columnas `from_` y 176 `to_` son espejos exactos\n",
    "2. Identificar y eliminar columnas redundantes (`team_id_to`, `competition_to`, `season_to`)\n",
    "3. Reordenar en bloques l√≥gicos y guardar parquet limpio (360 cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Paths (resolve Unicode dir name dynamically) --\n",
    "from pathlib import Path\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for _d in docs.iterdir():\n",
    "    if \"Jorge\" in _d.name and \"MacBook\" in _d.name and _d.is_dir():\n",
    "        RAW = _d / \"thesis_data\" / \"raw_data\"\n",
    "        PROCESSED = _d / \"thesis_data\" / \"processed_data\"\n",
    "        break\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "base = RAW / \"Transfers\"\n",
    "df = pd.read_parquet(base / \"male_transfers_model_2018_2025.parquet\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verificaci√≥n de alineaci√≥n from_ ‚Üî to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_cols = sorted([c for c in df.columns if c.startswith(\"from_\")])\n",
    "to_cols   = sorted([c for c in df.columns if c.startswith(\"to_\")])\n",
    "\n",
    "from_suffixes = sorted([c.replace(\"from_\", \"\", 1) for c in from_cols])\n",
    "to_suffixes   = sorted([c.replace(\"to_\", \"\", 1) for c in to_cols])\n",
    "\n",
    "print(f\"from_ columns: {len(from_cols)}\")\n",
    "print(f\"to_   columns: {len(to_cols)}\")\n",
    "print(f\"\\nSuffixes match perfectly: {from_suffixes == to_suffixes}\")\n",
    "\n",
    "only_from = sorted(set(from_suffixes) - set(to_suffixes))\n",
    "only_to   = sorted(set(to_suffixes) - set(from_suffixes))\n",
    "if only_from:\n",
    "    print(f\"\\n‚ö†Ô∏è Only in from_: {only_from}\")\n",
    "if only_to:\n",
    "    print(f\"\\n‚ö†Ô∏è Only in to_: {only_to}\")\n",
    "if not only_from and not only_to:\n",
    "    print(\"\\n‚úÖ Todas las 176 m√©tricas from_ tienen su espejo exacto en to_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clasificaci√≥n de sufijos en categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_suffix(s):\n",
    "    \"\"\"Clasifica un sufijo en: meta, per90, zscore, raw\"\"\"\n",
    "    if s in [\"team_id\", \"competition\", \"season\", \"position\", \"Minutes\"]:\n",
    "        return \"meta\"\n",
    "    if s.startswith(\"z_score_\"):\n",
    "        return \"zscore\"\n",
    "    if \"per 90\" in s:\n",
    "        return \"per90\"\n",
    "    return \"raw\"\n",
    "\n",
    "categories = {}\n",
    "for s in from_suffixes:\n",
    "    cat = classify_suffix(s)\n",
    "    categories.setdefault(cat, []).append(s)\n",
    "\n",
    "for cat in [\"meta\", \"raw\", \"per90\", \"zscore\"]:\n",
    "    items = categories.get(cat, [])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{cat.upper()} ({len(items)} columnas)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for s in sorted(items):\n",
    "        print(f\"  {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Columnas globales (metadata del jugador y del transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_global = [c for c in df.columns if not c.startswith(\"from_\") and not c.startswith(\"to_\")]\n",
    "print(f\"Columnas globales ({len(meta_global)}):\")\n",
    "for c in meta_global:\n",
    "    print(f\"  {c:30s} dtype={str(df[c].dtype):20s} nulls={df[c].isna().sum():>6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tabla de verificaci√≥n from ‚Üî to lado a lado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_rows = []\n",
    "for s in sorted(from_suffixes):\n",
    "    cat = classify_suffix(s)\n",
    "    fc = f\"from_{s}\"\n",
    "    tc = f\"to_{s}\"\n",
    "    alignment_rows.append({\n",
    "        \"suffix\": s,\n",
    "        \"category\": cat,\n",
    "        \"from_col\": fc,\n",
    "        \"from_dtype\": str(df[fc].dtype),\n",
    "        \"to_col\": tc,\n",
    "        \"to_dtype\": str(df[tc].dtype),\n",
    "        \"dtype_match\": str(df[fc].dtype) == str(df[tc].dtype)\n",
    "    })\n",
    "\n",
    "df_align = pd.DataFrame(alignment_rows)\n",
    "print(f\"Total pares from/to: {len(df_align)}\")\n",
    "print(f\"Dtypes coinciden en todos: {df_align['dtype_match'].all()}\")\n",
    "\n",
    "mismatches = df_align[~df_align[\"dtype_match\"]]\n",
    "if len(mismatches) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Dtype mismatches ({len(mismatches)}):\")\n",
    "    print(mismatches[[\"suffix\", \"from_dtype\", \"to_dtype\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos los pares from/to tienen el mismo dtype\")\n",
    "\n",
    "print(\"\\nüìä Conteo por categor√≠a:\")\n",
    "print(df_align[\"category\"].value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "df_align[[\"category\", \"from_col\", \"to_col\", \"from_dtype\", \"dtype_match\"]].sort_values([\"category\", \"from_col\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reordenar columnas y guardar parquet limpio\n",
    "\n",
    "Orden final:\n",
    "\n",
    "| Bloque | # Cols |\n",
    "|--------|--------|\n",
    "| Player metadata (`player_id`, `short_name`, `birth_date`, `player_season_age`, `transfer_type`) | 5 |\n",
    "| Transfer dates (`competition_start_date`, `first_played_date`, `last_played_date`) | 3 |\n",
    "| FROM metadata (team_id, competition, season, position, Minutes) | 5 |\n",
    "| FROM raw metrics | 50 |\n",
    "| FROM per 90 | 46 |\n",
    "| FROM z-scores | 75 |\n",
    "| TO metadata (team_id, competition, season, position, Minutes) | 5 |\n",
    "| TO raw metrics | 50 |\n",
    "| TO per 90 | 46 |\n",
    "| TO z-scores | 75 |\n",
    "| **TOTAL** | **360** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ordered column list\n",
    "player_meta = [\"player_id\", \"short_name\", \"birth_date\", \"player_season_age\", \"transfer_type\"]\n",
    "transfer_dates = [\"competition_start_date\", \"first_played_date\", \"last_played_date\"]\n",
    "\n",
    "# FROM sections\n",
    "from_meta = [f\"from_{s}\" for s in sorted(categories[\"meta\"])]\n",
    "from_raw  = [f\"from_{s}\" for s in sorted(categories[\"raw\"])]\n",
    "from_p90  = [f\"from_{s}\" for s in sorted(categories[\"per90\"])]\n",
    "from_z    = [f\"from_{s}\" for s in sorted(categories[\"zscore\"])]\n",
    "\n",
    "# TO sections (exact mirror)\n",
    "to_meta = [f\"to_{s}\" for s in sorted(categories[\"meta\"])]\n",
    "to_raw  = [f\"to_{s}\" for s in sorted(categories[\"raw\"])]\n",
    "to_p90  = [f\"to_{s}\" for s in sorted(categories[\"per90\"])]\n",
    "to_z    = [f\"to_{s}\" for s in sorted(categories[\"zscore\"])]\n",
    "\n",
    "ordered_cols = (\n",
    "    player_meta + transfer_dates +\n",
    "    from_meta + from_raw + from_p90 + from_z +\n",
    "    to_meta + to_raw + to_p90 + to_z\n",
    ")\n",
    "\n",
    "# Verify all columns accounted for\n",
    "missing = set(df.columns) - set(ordered_cols)\n",
    "extra   = set(ordered_cols) - set(df.columns)\n",
    "print(f\"Columnas en df pero no en orden: {missing if missing else '‚àÖ'}\")\n",
    "print(f\"Columnas en orden pero no en df: {extra if extra else '‚àÖ'}\")\n",
    "print(f\"Total: {len(ordered_cols)} (expected {len(df.columns)})\")\n",
    "assert len(missing) == 0 and len(extra) == 0 and len(ordered_cols) == len(df.columns)\n",
    "print(\"\\n‚úÖ Todas las columnas incluidas, sin duplicados ni faltantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder\n",
    "df_ordered = df[ordered_cols].copy()\n",
    "\n",
    "# Print final structure\n",
    "sections = [\n",
    "    (\"Player metadata\", player_meta),\n",
    "    (\"Transfer dates\", transfer_dates),\n",
    "    (\"FROM metadata\", from_meta),\n",
    "    (\"FROM raw metrics\", from_raw),\n",
    "    (\"FROM per 90\", from_p90),\n",
    "    (\"FROM z-scores\", from_z),\n",
    "    (\"TO metadata\", to_meta),\n",
    "    (\"TO raw metrics\", to_raw),\n",
    "    (\"TO per 90\", to_p90),\n",
    "    (\"TO z-scores\", to_z),\n",
    "]\n",
    "\n",
    "col_idx = 0\n",
    "print(f\"{'Bloque':<22s} {'Rango':<14s} {'# Cols':>6s}\")\n",
    "print(\"-\" * 44)\n",
    "for name, cols in sections:\n",
    "    start = col_idx\n",
    "    end = col_idx + len(cols) - 1\n",
    "    print(f\"{name:<22s} [{start:>3d} - {end:>3d}]  {len(cols):>5d}\")\n",
    "    col_idx += len(cols)\n",
    "print(\"-\" * 44)\n",
    "print(f\"{'TOTAL':<22s} {'':14s} {col_idx:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificaci√≥n final: from y to son espejos exactos en cada categor√≠a\n",
    "for cat_name, f_list, t_list in [\n",
    "    (\"meta\", from_meta, to_meta),\n",
    "    (\"raw\", from_raw, to_raw),\n",
    "    (\"per90\", from_p90, to_p90),\n",
    "    (\"zscore\", from_z, to_z)\n",
    "]:\n",
    "    f_suf = [c.replace(\"from_\", \"\", 1) for c in f_list]\n",
    "    t_suf = [c.replace(\"to_\", \"\", 1) for c in t_list]\n",
    "    match = f_suf == t_suf\n",
    "    print(f\"{cat_name:>8s}: {len(f_list)} from == {len(t_list)} to, orden id√©ntico: {match}\")\n",
    "\n",
    "print(\"\\n‚úÖ Estructura from/to perfectamente alineada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar\n",
    "out_path = base / \"male_transfers_model_2018_2025.parquet\"\n",
    "df_ordered.to_parquet(out_path, index=False)\n",
    "\n",
    "print(f\"Guardado: {out_path}\")\n",
    "print(f\"Shape:    {df_ordered.shape}\")\n",
    "print(f\"Tama√±o:   {out_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Verify\n",
    "df_check = pd.read_parquet(out_path)\n",
    "assert list(df_check.columns) == ordered_cols\n",
    "assert df_check.shape == df_ordered.shape\n",
    "print(f\"\\n‚úÖ Verificado: {df_check.shape[0]:,} rows √ó {df_check.shape[1]} cols, orden correcto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapshot: primeras columnas de cada secci√≥n\n",
    "print(\"Primeras columnas de cada bloque:\")\n",
    "for name, cols in sections:\n",
    "    preview = cols[:3]\n",
    "    print(f\"\\n  {name}:\")\n",
    "    for c in preview:\n",
    "        print(f\"    {c}\")\n",
    "    if len(cols) > 3:\n",
    "        print(f\"    ... ({len(cols) - 3} m√°s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}