{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge de Team Season Stats al dataset de Transfers\n",
    "\n",
    "Se cruza `Teams_stats/team_stats_season.parquet` con el dataset principal\n",
    "usando `(team_id, competition_id, season)` tanto para FROM como para TO.\n",
    "\n",
    "## Estructura del archivo fuente (77 cols, 36,228 rows)\n",
    "\n",
    "| Rango | Contenido | Acción |\n",
    "|-------|-----------|--------|\n",
    "| [0-2] `team_id`, `competition_id`, `season` | Join keys | Usados para el merge |\n",
    "| [3-76] 74 métricas del equipo | fouls_commited → xg_from_direct_attacks | **Merge** como `from_team_stats_*` y `to_team_stats_*` |\n",
    "\n",
    "**Total métricas:** 74 por lado (from/to) = **148 columnas nuevas**\n",
    "\n",
    "**Datos:** 7,529 equipos × 375 competiciones × 13 temporadas (2014-2026), sin duplicados en key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Paths (resolve Unicode dir name dynamically) --\n",
    "from pathlib import Path\n",
    "docs = Path(\"/Users/jorgepadilla/Documents\")\n",
    "for _d in docs.iterdir():\n",
    "    if \"Jorge\" in _d.name and \"MacBook\" in _d.name and _d.is_dir():\n",
    "        RAW = _d / \"thesis_data\" / \"raw_data\"\n",
    "        PROCESSED = _d / \"thesis_data\" / \"processed_data\"\n",
    "        break\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "ts = pd.read_parquet(base / \"Teams_stats\" / \"team_stats_season.parquet\")\n",
    "tf = pd.read_parquet(base / \"Transfers\" / \"male_transfers_model_2018_2025.parquet\")\n",
    "\n",
    "# Drop team stats cols if re-running\n",
    "old = [c for c in tf.columns if \"team_stats\" in c]\n",
    "if old:\n",
    "    tf = tf.drop(columns=old)\n",
    "    print(f\"Dropped {len(old)} existing team stats cols\")\n",
    "\n",
    "print(f\"Team stats: {ts.shape}\")\n",
    "print(f\"Transfers:  {tf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validación de estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_keys = [\"team_id\", \"competition_id\", \"season\"]\n",
    "metrics = ts.columns[3:].tolist()  # 74 métricas\n",
    "\n",
    "print(f\"Join keys:    {len(join_keys)} → {join_keys}\")\n",
    "print(f\"Métricas:     {len(metrics)} cols\")\n",
    "print(f\"Total:        {len(join_keys) + len(metrics)} / {len(ts.columns)}\")\n",
    "assert len(join_keys) + len(metrics) == len(ts.columns)\n",
    "\n",
    "# Granularity check\n",
    "combos = ts.groupby(join_keys).ngroups\n",
    "print(f\"\\nRows: {len(ts):,}\")\n",
    "print(f\"Unique (team_id, competition_id, season): {combos:,}\")\n",
    "print(f\"Duplicates: {ts.duplicated(subset=join_keys).sum()}\")\n",
    "assert combos == len(ts), \"Hay duplicados en la key!\"\n",
    "\n",
    "print(f\"\\nEquipos: {ts['team_id'].nunique():,}\")\n",
    "print(f\"Comps:   {ts['competition_id'].nunique()}\")\n",
    "print(f\"Seasons: {sorted(ts['season'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all metric columns\n",
    "print(f\"=== 74 TEAM METRICS ===\")\n",
    "for i, c in enumerate(metrics):\n",
    "    nulls = ts[c].isna().sum()\n",
    "    null_str = f\"  (nulls={nulls})\" if nulls > 0 else \"\"\n",
    "    print(f\"  [{i:>2d}] {c}{null_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparar y mergear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM merge\n",
    "ts_from = ts.rename(columns={m: f\"from_team_stats_{m}\" for m in metrics})\n",
    "ts_from = ts_from.rename(columns={\n",
    "    \"team_id\": \"from_team_id\",\n",
    "    \"competition_id\": \"from_competition\",\n",
    "    \"season\": \"from_season\"\n",
    "})\n",
    "ts_from[\"from_team_id\"] = ts_from[\"from_team_id\"].astype(\"int32\")\n",
    "ts_from[\"from_competition\"] = ts_from[\"from_competition\"].astype(\"int32\")\n",
    "ts_from[\"from_season\"] = ts_from[\"from_season\"].astype(\"int16\")\n",
    "\n",
    "n_before = len(tf)\n",
    "df = tf.merge(ts_from, on=[\"from_team_id\", \"from_competition\", \"from_season\"], how=\"left\")\n",
    "assert len(df) == n_before, f\"Row count changed! {n_before} -> {len(df)}\"\n",
    "\n",
    "from_cov = df[\"from_team_stats_goals\"].notna().sum()\n",
    "print(f\"FROM merge: {from_cov:,} / {n_before:,} ({from_cov/n_before*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO merge\n",
    "ts_to = ts.rename(columns={m: f\"to_team_stats_{m}\" for m in metrics})\n",
    "ts_to = ts_to.rename(columns={\n",
    "    \"team_id\": \"to_team_id\",\n",
    "    \"competition_id\": \"to_competition\",\n",
    "    \"season\": \"to_season\"\n",
    "})\n",
    "ts_to[\"to_team_id\"] = ts_to[\"to_team_id\"].astype(\"int32\")\n",
    "ts_to[\"to_competition\"] = ts_to[\"to_competition\"].astype(\"int32\")\n",
    "ts_to[\"to_season\"] = ts_to[\"to_season\"].astype(\"int16\")\n",
    "\n",
    "df = df.merge(ts_to, on=[\"to_team_id\", \"to_competition\", \"to_season\"], how=\"left\")\n",
    "assert len(df) == n_before, f\"Row count changed! {n_before} -> {len(df)}\"\n",
    "\n",
    "to_cov = df[\"to_team_stats_goals\"].notna().sum()\n",
    "print(f\"TO merge:   {to_cov:,} / {n_before:,} ({to_cov/n_before*100:.1f}%)\")\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "duped_cols = df.columns[df.columns.duplicated()].tolist()\n",
    "xy_cols = [c for c in df.columns if c.endswith(\"_x\") or c.endswith(\"_y\")]\n",
    "print(f\"Duplicate cols: {duped_cols if duped_cols else 'None'}\")\n",
    "print(f\"Merge artifacts: {xy_cols if xy_cols else 'None'}\")\n",
    "\n",
    "# Verify from/to mirror\n",
    "from_ts_suf = sorted([c.replace(\"from_team_stats_\", \"\") for c in df.columns if c.startswith(\"from_team_stats_\")])\n",
    "to_ts_suf = sorted([c.replace(\"to_team_stats_\", \"\") for c in df.columns if c.startswith(\"to_team_stats_\")])\n",
    "print(f\"from_team_stats: {len(from_ts_suf)} cols\")\n",
    "print(f\"to_team_stats:   {len(to_ts_suf)} cols\")\n",
    "print(f\"Perfect mirror:  {from_ts_suf == to_ts_suf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reordenar y guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns by section\n",
    "all_cols = list(df.columns)\n",
    "\n",
    "meta_cols = []\n",
    "wyscout_cols = []\n",
    "from_player_cols = []\n",
    "from_comp_cols = []\n",
    "from_ts_cols = []\n",
    "to_player_cols = []\n",
    "to_comp_cols = []\n",
    "to_ts_cols = []\n",
    "\n",
    "for c in all_cols:\n",
    "    if c.startswith(\"from_team_stats_\"):\n",
    "        from_ts_cols.append(c)\n",
    "    elif c.startswith(\"from_comp_\"):\n",
    "        from_comp_cols.append(c)\n",
    "    elif c.startswith(\"from_\"):\n",
    "        from_player_cols.append(c)\n",
    "    elif c.startswith(\"to_team_stats_\"):\n",
    "        to_ts_cols.append(c)\n",
    "    elif c.startswith(\"to_comp_\"):\n",
    "        to_comp_cols.append(c)\n",
    "    elif c.startswith(\"to_\"):\n",
    "        to_player_cols.append(c)\n",
    "    elif c.startswith(\"wyscout_\"):\n",
    "        wyscout_cols.append(c)\n",
    "    else:\n",
    "        meta_cols.append(c)\n",
    "\n",
    "ordered = (meta_cols + wyscout_cols +\n",
    "           from_player_cols + from_comp_cols + from_ts_cols +\n",
    "           to_player_cols + to_comp_cols + to_ts_cols)\n",
    "\n",
    "assert set(ordered) == set(all_cols) and len(ordered) == len(all_cols)\n",
    "df = df[ordered]\n",
    "\n",
    "# Section summary\n",
    "sections = [\n",
    "    (\"player_meta\", meta_cols),\n",
    "    (\"wyscout_player\", wyscout_cols),\n",
    "    (\"from_player\", from_player_cols),\n",
    "    (\"from_comp_meta\", from_comp_cols),\n",
    "    (\"from_team_stats\", from_ts_cols),\n",
    "    (\"to_player\", to_player_cols),\n",
    "    (\"to_comp_meta\", to_comp_cols),\n",
    "    (\"to_team_stats\", to_ts_cols),\n",
    "]\n",
    "\n",
    "idx = 0\n",
    "print(f\"{'Sección':<25s} {'Rango':<14s} {'#':>4s}  {'Coverage':>8s}\")\n",
    "print(\"-\" * 56)\n",
    "for name, cols in sections:\n",
    "    cov = df[cols[0]].notna().sum() / len(df) * 100\n",
    "    print(f\"{name:<25s} [{idx:>3d} - {idx+len(cols)-1:>3d}]  {len(cols):>4d}  {cov:>7.1f}%\")\n",
    "    idx += len(cols)\n",
    "print(\"-\" * 56)\n",
    "print(f\"{'TOTAL':<25s} {'':14s} {idx:>4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = base / \"Transfers\" / \"male_transfers_model_2018_2025.parquet\"\n",
    "df.to_parquet(out_path, index=False)\n",
    "\n",
    "print(f\"Guardado: {out_path}\")\n",
    "print(f\"Shape:    {df.shape}\")\n",
    "print(f\"Tamaño:   {out_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Verify\n",
    "df_check = pd.read_parquet(out_path)\n",
    "assert list(df_check.columns) == ordered and df_check.shape == df.shape\n",
    "print(f\"Verificado: {df_check.shape[0]:,} rows × {df_check.shape[1]} cols\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}